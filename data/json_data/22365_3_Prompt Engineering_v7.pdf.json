[{"data": "## Text Content  \nPrompt Engineering  \nAuthor: Lee Boonstra\n\n## Image Description  \nThe image is a cover page featuring the title \"Prompt Engineering\" in bold, large text, with the author \"Lee Boonstra\" credited below in smaller, gray text. In the bottom-right corner, there is a large, faceted, geometric crystal rendered in shades of blue, purple, and black, giving a futuristic and polished appearance. The Google logo appears in full color at the bottom left, indicating the association or publication by Google. The overall design is modern and clean, with a white background and minimalistic layout.\n\n## Table Summary  \n*No tables present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Code  \n*No code present in the image.*\n\n## Formulas  \n*No formulas present in the image.*\n\n## Flowchart Description  \n*No flowcharts present in the image.*", "page": "1"}, {"data": "## Text Content  \nPrompt Engineering\n\n**Acknowledgements**\n\n**Content contributors**  \nMichael Sherman  \nYuan Cao  \nErick Armbrust  \nAnant Nawalgaría  \nAntonio Gulli  \nSimone Cammel  \n\n**Curators and Editors**  \nAntonio Gulli  \nAnant Nawalgaría  \nGrace Mollison  \n\n**Technical Writer**  \nJoey Haymaker  \n\n**Designer**  \nMichael Lanning  \n\nFebruary 2025\n\n## Table Summary  \n*No table is present in this image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe right side of the page features a large, glossy, multifaceted geometric object resembling a gemstone or crystal. The object displays reflective surfaces in shades of blue, black, and violet, giving it a polished and abstract appearance. The design is clean and modern, serving as a visual accent to the acknowledgements section.\n\n## Code  \n*No code is present in this image.*\n\n## Formulas  \n*No mathematical formulas are present in this image.*\n\n## Flowchart Description  \n*No flowchart is present in this image.*", "page": "2"}, {"data": "## Text Content  \nTable of contents\n\nIntroduction .................................................................................... 6  \nPrompt engineering ...................................................................... 7  \nLLM output configuration ....................................................... 8  \n  Output length ......................................................................... 8  \n  Sampling controls ................................................................. 9  \n    Temperature ................................................................. 9  \n    Top-K and top-P .......................................................... 10  \n  Putting it all together ......................................................... 11  \nPrompting techniques ................................................................ 13  \n  General prompting / zero shot .......................................... 13  \n  One-shot & few-shot ......................................................... 15  \nSystem, contextual and role prompting ............................. 18  \n  System prompting ............................................................... 19  \n  Role prompting .................................................................... 21  \n  Contextual prompting ......................................................... 23  \n\n## Table Summary  \nThis table of contents outlines the structure of a document focused on prompt engineering and techniques for configuring large language model (LLM) outputs. It includes sections on basic concepts, technical configurations, and various prompting strategies.\n\n## Table Data  \n- **Introduction**: Page 6  \n- **Prompt engineering**: Page 7  \n- **LLM output configuration**: Page 8  \n  - Output length: Page 8  \n  - Sampling controls: Page 9  \n    - Temperature: Page 9  \n    - Top-K and top-P: Page 10  \n  - Putting it all together: Page 11  \n- **Prompting techniques**: Page 13  \n  - General prompting / zero shot: Page 13  \n  - One-shot & few-shot: Page 15  \n- **System, contextual and role prompting**: Page 18  \n  - System prompting: Page 19  \n  - Role prompting: Page 21  \n  - Contextual prompting: Page 23  \n\n## Image Description  \nThe image displays the table of contents for a document on prompt engineering and LLM prompt techniques. The layout is clean, with section titles and corresponding page numbers aligned for easy navigation. A geometric, multi-colored abstract shape appears in the bottom left corner, adding a modern visual element to the otherwise minimal design.\n\n## Code  \nN/A\n\n## Formulas  \nN/A\n\n## Flowchart Description  \nN/A", "page": "3"}, {"data": "## Text Content  \nStep-back prompting .................................................................................... 25  \nChain of Thought (CoT) ............................................................................. 29  \nSelf-consistency ............................................................................................. 32  \nTree of Thoughts (ToT) .............................................................................. 36  \nReAct (reason & act) .................................................................................. 37  \nAutomatic Prompt Engineering ............................................................ 40  \nCode prompting ............................................................................................ 42  \n  Prompts for writing code ...................................................................... 42  \n  Prompts for explaining code ............................................................... 44  \n  Prompts for translating code ............................................................... 46  \n  Prompts for debugging and reviewing code ................................. 48  \n  What about multimodal prompting? ................................................. 54  \n\nBest Practices .................................................................................................. 54  \n  Provide examples ..................................................................................... 54  \n  Design with simplicity ............................................................................. 55  \n  Be specific about the output .................................................................. 56  \n  Use Instructions over Constraints ....................................................... 56  \n  Control the max token length ............................................................. 58  \n  Use variables in prompts ....................................................................... 58  \n  Experiment with input formats and writing styles .......................... 59  \n  For few-shot prompting with classification tasks, mix up the classes ..... 59  \n  Adapt to model updates ......................................................................... 60  \n  Experiment with output formats ......................................................... 60  \n\n## Table Summary  \nThis page appears to be a table of contents from a document or manual, listing advanced prompting techniques and best practices related to prompting models, along with their corresponding page numbers.\n\n## Table Data  \n- **Prompting Techniques and Topics (with page numbers):**\n  - Step-back prompting (p. 25)\n  - Chain of Thought (CoT) (p. 29)\n  - Self-consistency (p. 32)\n  - Tree of Thoughts (ToT) (p. 36)\n  - ReAct (reason & act) (p. 37)\n  - Automatic Prompt Engineering (p. 40)\n  - Code prompting (p. 42)\n    - Prompts for writing code (p. 42)\n    - Prompts for explaining code (p. 44)\n    - Prompts for translating code (p. 46)\n    - Prompts for debugging and reviewing code (p. 48)\n    - What about multimodal prompting? (p. 54)\n- **Best Practices (with page numbers):**\n  - Provide examples (p. 54)\n  - Design with simplicity (p. 55)\n  - Be specific about the output (p. 56)\n  - Use Instructions over Constraints (p. 56)\n  - Control the max token length (p. 58)\n  - Use variables in prompts (p. 58)\n  - Experiment with input formats and writing styles (p. 59)\n  - For few-shot prompting with classification tasks, mix up the classes (p. 59)\n  - Adapt to model updates (p. 60)\n  - Experiment with output formats (p. 60)\n\n## Image Description  \nThe image displays a minimalistic, well-organized table of contents, likely from a guide or manual on prompt engineering for language models. The contents are divided into sections covering advanced prompting strategies (such as Chain of Thought, Tree of Thoughts, and ReAct), specific prompting for code, and a collection of best practices for effective prompt design. Each topic is followed by a dotted line leading to its corresponding page number, and subtopics are indented for clarity. The layout is clean with ample whitespace, making it easy to read and navigate.", "page": "4"}, {"data": "## Text Content  \nJSON Repair  \nWorking with Schemas  \nExperiment together with other prompt engineers  \nCoT Best practices  \nDocument the various prompt attempts  \nSummary  \nEndnotes  \n\n## Table Summary  \nThis segment appears to be a section of a table of contents from a document or book, listing chapter or section titles along with their corresponding page numbers.\n\n## Table Data  \n- **JSON Repair** — page 61  \n- **Working with Schemas** — page 62  \n- **Experiment together with other prompt engineers** — page 63  \n- **CoT Best practices** — page 64  \n- **Document the various prompt attempts** — page 64  \n- **Summary** — page 66  \n- **Endnotes** — page 68  \n\n## Image Description  \nThe image displays a portion of a table of contents, showing section titles and their associated page numbers. The layout uses dotted leader lines to connect each section title to its page number. The content is centered on the page with ample white space, indicating it is likely the end of the table of contents.\n\n## Code  \n_No code present in the image._\n\n## Formulas  \n_No formulas present in the image._\n\n## Flowchart Description  \n_No flowcharts present in the image._", "page": "5"}, {"data": "## Text Content  \n\"You don’t need to be a data scientist or a machine learning engineer – everyone can write a prompt.\"\n\n**Introduction**\n\nWhen thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don’t need to be a data scientist or a machine learning engineer – everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide meaningful output.\n\n## Image Description  \nThe image consists of a modern, minimalistic page layout titled \"Prompt Engineering.\" The main visual feature is a large, highlighted quote in grey text with a vertical red accent bar:  \n\"You don’t need to be a data scientist or a machine learning engineer – everyone can write a prompt.\"\n\nBelow the quote, the page features a bolded \"Introduction\" heading followed by a detailed paragraph explaining the fundamentals of prompt engineering. The paragraph emphasizes that anyone can write a prompt for large language models, but that the process of crafting effective prompts can be complex and involves many considerations such as model choice, data, word selection, tone, and structure. The bottom of the page includes a small date: \"February 2025\" and a page number \"6\". \n\n## Table Summary  \n*No tables are present in this image.*\n\n## Table Data  \n*No table data to report.*\n\n## Code  \n*No code is present in this image.*\n\n## Formulas  \n*No formulas are present in this image.*\n\n## Flowchart Description  \n*No flowcharts are present in this image.*", "page": "6"}, {"data": "## Text Content  \nWhen you chat with the Gemini chatbot, you basically write prompts, however this whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using the API, because by prompting the model directly you will have access to the configuration such as temperature etc.\n\nThis whitepaper discusses prompt engineering in detail. We will look into the various prompting techniques to help you getting started and share tips and best practices to become a prompting expert. We will also discuss some of the challenges you can face while crafting prompts.\n\n## Prompt engineering\n\nRemember how an LLM works; it’s a prediction engine. The model takes sequential text as an input and then predicts what the following token should be, based on the data it was trained on. The LLM is operationalized to do this over and over again, adding the previously predicted token to the end of the sequential text for predicting the following token. The next token prediction is based on the relationship between what’s in the previous tokens and what the LLM has seen during its training.\n\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence of tokens. Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt’s writing style and structure in relation to the task. In the context of natural language processing and LLMs, a prompt is an input provided to the model to generate a response or prediction.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No tables are present in the image.*\n\n## Image Description  \nThe image is a digital page from a whitepaper titled \"Prompt Engineering.\" The page contains explanatory text about prompt engineering, how large language models (LLMs) work, and the process of designing effective prompts. There are no illustrations, tables, or flowcharts—just formatted text emphasizing the nature and importance of prompt engineering for LLMs like Gemini.\n\n## Code  \n*No code blocks are present in the image.*\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "7"}, {"data": "## Text Content  \nThese prompts can be used to achieve various kinds of understanding and generation tasks such as text summarization, information extraction, question and answering, text classification, language or code translation, code generation, and code documentation or reasoning.\n\nPlease feel free to refer to Google’s prompting guides²,³ with simple and effective prompting examples.\n\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be optimized for your specific model, regardless of whether you use Gemini language models in Vertex AI, GPT, Claude, or an open source model like Gemma or LLaMA.\n\nBesides the prompt, you will also need to tinker with the various configurations of a LLM.\n\n## LLM output configuration\n\nOnce you choose your model you will need to figure out the model configuration. Most LLMs come with various configuration options that control the LLM’s output. Effective prompt engineering requires setting these configurations optimally for your task.\n\n### Output length\n\nAn important configuration setting is the number of tokens to generate in a response. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.\n\n## Table Summary  \n_No tables are present in the image._\n\n## Table Data  \n_No tables are present in the image._\n\n## Image Description  \nThe image is a page from a document or a guide on prompt engineering for large language models (LLMs). It discusses the types of tasks achievable with prompts, provides guidance on using Google’s prompting examples, describes the importance of model selection, and emphasizes the need to configure LLM settings appropriately. The section specifically highlights \"LLM output configuration,\" focusing on the significance of setting the output length parameter.\n\n## Code  \n_No code is present in the image._\n\n## Formulas  \n_No formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "8"}, {"data": "## Text Content  \nReducing the output length of the LLM doesn’t cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more tokens once the limit is reached. If your needs require a short output length, you’ll also possibly need to engineer your prompt to accommodate.\n\nOutput length restriction is especially important for some LLM prompting techniques, like ReAct, where the LLM will keep emitting useless tokens after the response you want.\n\nBe aware, generating more tokens requires more computation from the LLM, leading to higher energy consumption and potentially slower response times, which leads to higher costs.\n\n### Sampling controls\n\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the next token could be, with each token in the LLM’s vocabulary getting a probability. Those token probabilities are then sampled to determine what the next produced token will be. Temperature, top-K, and top-P are the most common configuration settings that determine how predicted token probabilities are processed to choose a single output token.\n\n### Temperature\n\nTemperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is\n\n## Table Summary  \n*There are no tables present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe image is a page from a document titled \"Prompt Engineering.\" It contains detailed text descriptions regarding the impact of output length restrictions on LLMs, the importance of output length for certain prompting techniques, and an explanation of sampling controls and temperature parameter in large language models. There are no photos, illustrations, or diagrams—only structured text.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "9"}, {"data": "## Text Content  \ndeterministic: the highest probability token is always selected (though note that if two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0).\n\nTemperatures close to the max tend to create more random output. And as temperature gets higher and higher, all tokens become equally likely to be the next predicted token.\n\nThe Gemini temperature control can be understood in a similar way to the softmax function used in machine learning. A low temperature setting mirrors a low softmax temperature (T), emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a rigid, precise temperature may not be essential like for example when experimenting with creative outputs.\n\n### Top-K and top-P\n\nTop–K and top–P (also known as nucleus sampling) are two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities. Like temperature, these sampling settings control the randomness and diversity of generated text.\n\n- **Top–K** sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top–K, the more creative and varied the model’s output; the lower top–K, the more restive and factual the model’s output. A top–K of 1 is equivalent to greedy decoding.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No tables are present in the image.*\n\n## Image Description  \nThe image is a page from a document titled \"Prompt Engineering.\" It discusses the concepts of temperature, top-K, and top-P sampling in the context of large language models (LLMs). The section contains clear headings, explanatory paragraphs, and a bulleted list that explains the difference between top-K and top-P sampling techniques.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No explicit mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "10"}, {"data": "## Text Content  \n- **Top-P sampling** selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM's vocabulary).\n\nThe best way to choose between top-K and top-P is to experiment with both methods (or both together) and see which one produces the results you are looking for.\n\n### Putting it all together\n\nChoosing between top-K, top-P, temperature, and the number of tokens to generate, depends on the specific application and desired outcome, and the settings all impact one another. It’s also important to make sure you understand how your chosen model combines the different sampling settings together.\n\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet both the top-K and top-P criteria are candidates for the next predicted token, and then temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If only top-K or top-P is available, the behavior is the same but only the one top-K or P setting is used.\n\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then randomly selected from to produce a single next predicted token.\n\nAt extreme settings of one sampling configuration value, that one sampling setting either cancels out other configuration settings or becomes irrelevant.\n\n## Table Summary  \n_No tables are present in the image._\n\n## Table Data  \n_No tables are present in the image._\n\n## Image Description  \nThe image is a page from a document titled \"Prompt Engineering.\" It describes the concepts of top-P sampling and the interaction between various sampling parameters (top-K, top-P, temperature) when generating text with language models. There is no graphical illustration, only formatted text content.\n\n## Code  \n_No code is present in the image._\n\n## Formulas  \n_No formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "11"}, {"data": "## Text Content  \n- If you set temperature to 0, top-K and top-P become irrelevant—the most probable token becomes the next token predicted. If you set temperature extremely high (above 1–generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria are then randomly sampled to choose a next predicted token.\n\n- If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the top-K criteria, and that token is the next predicted token. If you set top-K extremely high, like to the size of the LLM’s vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out.\n\n- If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out.\n\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so. If you want especially creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. Finally, if your task always has a single correct answer (e.g., answering a math problem), start with a temperature of 0.\n\n**NOTE:** With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM might generate text that is less relevant.\n\n**WARNING:** Have you ever seen a response ending with a large amount of filler words? This is also known as the \"repetition loop bug\", which is a common issue in Large Language Models where the model gets stuck in a cycle, repeatedly generating the same (filler) word, phrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe page contains text guidance on the effects of temperature, top-K, and top-P parameters in Large Language Model (LLM) sampling. It explains what happens when each parameter is set to certain extremes, offers recommended starting values for different levels of creativity, and provides a note about output relevance. There is also a warning about the \"repetition loop bug,\" where models repeat filler content due to poor parameter selection.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No mathematical formulas are present.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "12"}, {"data": "## Text Content  \ntop-p settings. This can occur at both low and high temperature settings, though for different reasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the highest probability path, which can lead to a loop if that path revisits previously generated text. Conversely, at high temperatures, the model’s output becomes excessively random, increasing the probability that a randomly chosen word or phrase will, by chance, lead back to a prior state, creating a loop due to the vast number of available options. In both cases, the model’s sampling process gets “stuck,” resulting in monotonous and unhelpful output until the output window is filled. Solving this often requires careful tinkering with temperature and top-k/top-p values to find the optimal balance between determinism and randomness.\n\n## Prompting techniques\n\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can understand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your prompt text, the better it is for the LLM to predict the next likely text. Additionally, specific techniques that take advantage of how LLMs are trained and how LLMs work will help you get the relevant results from LLMs.\n\nNow that we understand what prompt engineering is and what it takes, let’s dive into some examples of the most important prompting techniques.\n\n## General prompting / zero shot\n\nA zero-shot prompt is the simplest type of prompt. It only provides a description of a task and some text for the LLM to get started with. This input could be anything: a question, a start of a story, or instructions. The name zero-shot stands for ‘no examples’.\n\n---\n\n*No table, code, formula, or flowchart content was present in the image. The image consists solely of well-structured text, organized above for readability.*", "page": "13"}, {"data": "## Text Content  \nLet’s use Vertex AI Studio (for Language) in Vertex AI, which provides a playground to test prompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews.\n\nThe table format as used below is a great way of documenting prompts. Your prompts will likely go through many iterations before they end up in a codebase, so it’s important to keep track of your prompt engineering work in a disciplined, structured way. More on this table format, the importance of tracking prompt engineering work, and the prompt development process is in the Best Practices section later in this chapter (“Document the various prompt attempts”).\n\nThe model temperature should be set to a low number, since no creativity is needed, and we use the gemini-pro default top-K and top-P values, which effectively disable both settings (see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words *disturbing* and *masterpiece* should make the prediction a little more complicated, as both words are used in the same sentence.\n\n## Table Summary  \nNo table is present on this page, but the text references \"Table 1,\" which is described as providing an example zero-shot prompt to classify movie reviews and as a recommended format for documenting prompt engineering work.\n\n## Table Data  \nNo table data is shown on this page.\n\n## Image Description  \nThere are no graphical images or illustrations on this page.\n\n## Code  \nNo code is present on this page.\n\n## Formulas  \nNo formulas are present on this page.\n\n## Flowchart Description  \nNo flowcharts are present on this page.", "page": "14"}, {"data": "## Text Content  \nWhen zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, which leads to “one-shot” and “few-shot” prompting. General prompting / zero shot\n\n### One-shot & few-shot\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can help the model understand what you are asking for. Examples are especially useful when you want to steer the model to a certain output structure or pattern.\n\nA one-shot prompt provides a single example, hence the name one-shot. The idea is the model has an example it can imitate to best complete the task.\n\nA few-shot prompt provides multiple examples to the model. This approach shows the model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples of the desired pattern increases the chance the model follows the pattern.\n\n## Table Summary  \nThe table demonstrates an example of zero-shot prompting for movie review classification using a language model. It outlines the settings and prompt used, as well as the model's output.\n\n## Table Data  \n- **Name**: 1_1_movie_classification  \n- **Goal**: Classify movie reviews as positive, neutral, or negative.  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 5  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt**:  \n  Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.  \n  Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece.  \n  Sentiment:  \n- **Output**: POSITIVE\n\n## Image Description  \nThe image contains a table labeled \"Table 1. An example of zero-shot prompting,\" which presents the configuration and result of a zero-shot prompt for classifying movie reviews. The table lists model parameters and a specific prompt, followed by the model's output. Below the table, explanatory text describes one-shot and few-shot prompting, including definitions and how these techniques help guide AI models by providing examples. The content is structured clearly for easy reading, with bolded section headers and explanatory paragraphs.\n\n## Code  \n```\nClassify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece.\nSentiment:\n```\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "15"}, {"data": "## Text Content  \nThe number of examples you need for few-shot prompting depends on a few factors, including the complexity of the task, the quality of the examples, and the capabilities of the generative AI (gen AI) model you are using. As a general rule of thumb, you should use at least three to five examples for few-shot prompting. However, you may need to use more examples for more complex tasks, or you may need to use fewer due to the input length limitation of your model.\n\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model configuration settings as before, other than increasing the token limit to accommodate the need for a longer response.\n\n## Table Summary  \nThe table demonstrates the configuration settings and prompt for a few-shot learning example using the gemini-pro model to parse customer pizza orders into JSON format.\n\n## Table Data  \n- **Goal**: Parse pizza orders to JSON  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 250  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt**:  \n  - Parse a customer's pizza order into valid JSON.\n  - Example:\n    - Input: \"I want a small pizza with cheese, tomato sauce, and pepperoni.\"\n    - JSON Response:\n      ```\n      {\n        \"size\": \"small\",\n        \"type\": \"normal\",\n        \"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n      }\n      ```\n  - (Further content indicated as continuing on the next page.)\n\n## Image Description  \nThe image shows a section from a technical document on prompt engineering. It contains a descriptive paragraph about few-shot prompting, followed by a table (Table 2) with configuration parameters for a gemini-pro model. The table includes fields for goal, model, temperature, token limit, top-K, top-P, and a sample prompt for converting pizza orders into JSON format, complete with an input example and corresponding JSON output.\n\n## Code  \n```\nParse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n...\n{\n  \"size\": \"small\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n...\n```\n", "page": "16"}, {"data": "## Text Content  \nEXAMPLE:  \nCan I get a large pizza with tomato sauce, basil and mozzarella  \n\n{  \n\"size\": \"large\",  \n\"type\": \"normal\",  \n\"ingredients\": [[\"tomato sauce\", \"basil\", \"mozzarella\"]]  \n}  \n\nNow, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple.  \n\nJSON Response:  \n\nWhen you choose examples for your prompt, use examples that are relevant to the task you want to perform. The examples should be diverse, of high quality, and well written. One small mistake can confuse the model and will result in undesired output.  \n\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to include edge cases in your examples. Edge cases are inputs that are unusual or unexpected, but that the model should still be able to handle.\n\n## Table Summary  \nThe table provides an example of few-shot prompting in which a user asks for different types of pizzas, and the desired output is shown in JSON format. It demonstrates how to structure prompts and their corresponding outputs for an AI model.\n\n## Table Data  \n- **Prompt Example**:  \n  - The user requests a large pizza with tomato sauce, basil, and mozzarella, formatted in JSON as:  \n    - Size: large  \n    - Type: normal  \n    - Ingredients: tomato sauce, basil, mozzarella  \n  - Next, the user requests a large pizza with the first half cheese and mozzarella, and the other half with tomato sauce, ham, and pineapple.\n  - The desired JSON output for this complex order should reflect:  \n    - Size: large  \n    - Type: half-half  \n    - Ingredients:  \n      - First half: cheese, mozzarella  \n      - Second half: tomato sauce, ham, pineapple\n\n## Image Description  \nThe image displays a table labeled \"Table 2. An example of few-shot prompting.\" The table contains two rows:\n- The first row, labeled \"Prompt,\" gives example natural language pizza orders and their corresponding JSON representations.\n- The second row, labeled \"Output,\" shows the expected JSON response for a more complex pizza order.  \nBelow the table, guidance text explains best practices for prompt engineering, emphasizing the need for relevant, diverse, and well-written examples, as well as the importance of including edge cases.\n\n## Code  \n```json\n{\n  \"size\": \"large\",\n  \"type\": \"half-half\",\n  \"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\", \"ham\", \"pineapple\"]]\n}\n```\n", "page": "17"}, {"data": "## Text Content  \nSystem, contextual and role prompting\n\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on different aspects:\n\n- **System prompting** sets the overall context and purpose for the language model. It defines the ‘big picture’ of what the model should be doing, like translating a language, classifying a review etc.\n\n- **Contextual prompting** provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what’s being asked and tailor the response accordingly.\n\n- **Role prompting** assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.\n\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a prompt that assigns a role to the system, can also have a context.\n\nHowever, each type of prompt serves a slightly different primary purpose:\n\n- System prompt: Defines the model’s fundamental capabilities and overarching purpose.\n- Contextual prompt: Provides immediate, task-specific information to guide the response. It’s highly specific to the current task or input, which is dynamic.\n- Role prompt: Frames the model’s output style and voice. It adds a layer of specificity and personality.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No tables are present in the image.*\n\n## Image Description  \nThe image is a page from a document or presentation titled \"Prompt Engineering.\" It is focused on explaining the differences between system, contextual, and role prompting techniques for large language models. The layout is clean with hierarchical headers and bulleted lists to clarify each concept and their primary purposes. There are no visual graphics, only organized text.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "18"}, {"data": "## Text Content  \nDistinguishing between system, contextual, and role prompts provides a framework for designing prompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type influences the language model’s output.\n\nLet’s dive into these three different kinds of prompts.\n\n**System prompting**\n\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I increased the temperature to get a higher creativity level, and I specified a higher token limit. However, because of my clear instruction on how to return the output the model didn’t return extra text.\n\nSystem prompts can be useful for generating output that meets specific requirements. The name ‘system prompt’ actually stands for ‘providing an additional task to the system’. For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language, or you could use a system prompt to return a certain structure. Have a look into Table 4, where I return the output in JSON format.\n\n## Table Summary  \nTable 3 demonstrates an example of system prompting, where specific instructions are provided to control how the output is returned by the language model. The table outlines the configuration and input used for a movie review classification task, along with the model’s response.\n\n## Table Data  \n- **Goal**: The objective is to classify movie reviews as positive, neutral, or negative.\n- **Model**: The model used is \"gemini-pro\".\n- **Temperature**: Set to 1, which increases creativity in responses.\n- **Token Limit**: Set to 5, restricting the length of the output.\n- **Top-K**: Value is 40, influencing the randomness of word selection.\n- **Top-P**: Value is 0.8, controlling the cumulative probability for token sampling.\n- **Prompt**: The instruction is to classify movie reviews as positive, neutral, or negative, returning only the label in uppercase. The review provided is:  \n  \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n- **Output**: The model returns \"NEGATIVE\".\n\n## Image Description  \nThe image is a textbook-style page discussing prompt engineering, specifically focusing on system prompting. It contains a formatted table (Table 3) that details parameters and results for a system prompt used to classify a movie review. The layout is clean, with bold section headers and explanatory text to guide the reader through the technical setup and its implications.\n\n## Code  \n```\nClassify movie reviews as positive, neutral or negative. Only return the label in uppercase.\nReview: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\nSentiment:\n```\n\n## Flowchart Description  \n*No flowchart is present in the image.*\n\n## Formulas  \n*No mathematical formulas are present in the image.*", "page": "19"}, {"data": "## Text Content  \nClassify movie reviews as positive, neutral or negative, return JSON.\n\nReview: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n\nSchema:  \n...  \nMOVIE:  \n{  \n  \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\",  \n  \"name\": String  \n}  \nMOVIE REVIEWS:  \n{  \n  \"movie_reviews\": [MOVIE]  \n}  \n...  \nJSON Response:\n\n## Table Summary  \nThe table provides an example of system prompting for classifying movie reviews as positive, neutral, or negative, and returning results in JSON format. It specifies the prompt structure, the schema for the expected response, and the output generated by the system.\n\n## Table Data  \n- **Goal**: Classify movie reviews as positive, neutral or negative, and return the result in JSON format.\n- **Model**: gemini-pro\n- **Temperature**: 1\n- **Token Limit**: 1024\n- **Top-K**: 40\n- **Top-P**: 0.8\n- **Prompt**:  \n  - Instruction to classify movie reviews with a JSON schema.\n  - Provides a sample review:  \n    \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n  - Details the JSON schema for output (fields: \"sentiment\" and \"name\").\n- **Output**:  \n  - Returns a JSON object listing the review for \"Her\" with a \"NEGATIVE\" sentiment.\n\n## Image Description  \nThe image displays a structured table detailing an example of prompt engineering for a movie review sentiment classification system. The table includes columns for goal, model, temperature, token limit, top-K, top-P, prompt instructions, and the expected JSON output. Below the table is a caption: \"Table 4. An example of system prompting with JSON format.\" The page is dated February 2025 and is from a document on prompt engineering.\n\n## Code  \n```json\n{\n  \"movie_reviews\": [\n    {\n      \"sentiment\": \"NEGATIVE\",\n      \"name\": \"Her\"\n    }\n  ]\n}\n```\n", "page": "20"}, {"data": "## Text Content  \nThere are some benefits in returning JSON objects from a prompt that extracts data. In a real-world application I don’t need to manually create this JSON format, I can already return the data in a sorted order (very handy when working with datetime objects), but most importantly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations.\n\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply add an additional line to your prompt like: ‘You should be respectful in your answer.’.\n\n### Role prompting\n\nRole prompting is a technique in prompt engineering that involves assigning a specific role to the gen AI model. This can help the model to generate more relevant and informative output, as the model can craft its responses to the specific role that it has been assigned.\n\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten teacher, or a motivational speaker. Once the model has been assigned a role, you can then give it prompts that are specific to that role. For example, you could prompt a teacher to create a lesson plan that they can then review. Have a look into Table 5, where the model takes the role of a travel guide.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe page contains only text content, with no visual elements such as illustrations, charts, or photographs.\n\n## Code  \n*No code blocks are present in the image.*\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "21"}, {"data": "## Text Content  \nThe above example shows an example of taking the role of a travel agent. When you would change the role to a teacher of geography, you would notice that you will receive a different response.\n\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you’re looking for to improve the quality, relevance, and effectiveness of your output.\n\n## Table Summary  \nTable 5 demonstrates an example of role prompting for an AI model. It outlines how to instruct the model to act as a travel guide and provide museum suggestions in Amsterdam. The table details the prompt setup, model parameters, and the AI's response.\n\n## Table Data  \n- **Goal**: Act as travel guide and provide 3 travel suggestions  \n- **Model**: gemini-pro  \n- **Temperature**: 1  \n- **Token Limit**: 1024  \n- **Top-K**: 40  \n- **Top-P**: 0.8  \n- **Prompt**:  \n  I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. In some cases, I will also give you the type of places I will visit.  \n  My suggestion: \"I am in Amsterdam and I want to visit only museums.\"  \n  Travel Suggestions:  \n- **Output**:  \n  1. Rijksmuseum: World-renowned museum, known for Dutch masterpieces like Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of paintings and artifacts showcasing Dutch art and history.  \n  2. Van Gogh Museum: Immerse yourself in the captivating works of Vincent van Gogh. The museum houses the largest collection of his paintings and drawings, including “The Starry Night” and “Sunflowers.”  \n  3. Stedelijk Museum Amsterdam: Discover modern and contemporary art from around the world. The museum’s collection includes works by Picasso, Kandinsky, and Rietveld, housed in a striking modern building.\n\n## Image Description  \nThe image displays a structured table labeled \"Table 5. An example of role prompting.\" The table outlines how to set up a prompt for an AI model (gemini-pro) to act as a travel guide, specifying parameters such as temperature, token limit, Top-K, and Top-P. The example prompt asks for museum recommendations in Amsterdam, and the AI provides three detailed suggestions, each highlighting the distinct features of major museums in the city.\n\n## Code  \n```\nI want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. In some cases, I will also give you the type of places I will visit.\nMy suggestion: \"I am in Amsterdam and I want to visit only museums.\"\nTravel Suggestions:\n```\n\n## Flowchart Description  \nNo flowchart is present in the image.\n\n## Formulas  \nNo mathematical formulas are present in the image.", "page": "22"}, {"data": "## Text Content  \nHere are some styles you can choose from which I find effective:  \nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive\n\nLet’s change our prompt in Table 6 to include a humorous and inspirational style.\n\nContextual prompting\n\nBy providing contextual prompts, you can help ensure that your AI interactions are as seamless and efficient as possible. The model will be able to more quickly understand your request and be able to generate more accurate and relevant responses, as you can see in the example of Table 7.\n\n## Table Summary  \nTable 6 provides an example of role prompting using a humorous and inspirational tone. The table shows how a prompt to act as a travel guide can be crafted, and the style is reflected in the AI's output, which suggests three places to visit in Manhattan with playful and motivational language.\n\n## Table Data  \n- **Prompt**:  \n  I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me in a humorous style.  \n  My suggestion: \"I am in Manhattan.\"  \n  Travel Suggestions:\n\n- **Output**:  \n  1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire State Building and bask in the glory of Manhattan’s skyline. Prepare to feel like King Kong atop the Big Apple, minus the giant ape-sized banana.\n  2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum of Modern Art (MoMA). Gaze upon masterpieces that will boggle your mind and make you question whether your stick-figure drawings have any artistic merit.\n  3. Shop 'Til You Drop on Fifth Avenue: Indulge in some retail therapy on the iconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or actually shop) at designer boutiques that will make your wallet cry. But hey, you’re in Manhattan, so you might as well embrace the fabulousness!\n\n## Image Description  \nThe image displays a page from a guide on prompt engineering. It features a short list of effective descriptive styles for prompts, a table labeled \"Table 6\" showing an example of role prompting in a humorous and inspirational style, and a section introducing contextual prompting. The table presents a travel guide prompt and the AI's playful responses for visiting Manhattan.\n\n## Code  \n_No code is present in the image._\n\n## Formulas  \n_No formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "23"}, {"data": "## Text Content  \nContext: You are writing for a blog about retro 80's arcade video games.  \nSuggest 3 topics to write an article about with a few lines of description of what this article should contain.\n\n1. **The Evolution of Arcade Cabinet Design:**  \n   This article would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s to the sleek, neon-lit designs of the 1980s and beyond.\n\n2. **Blast From The Past: Iconic Arcade Games of The 80’s:**  \n   - This article would feature a glimpse into the history, impact, and legacy of some of the most iconic and influential arcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm.\n\n3. **The Rise and Retro Revival of Pixel Art:**  \n   - This article would delve into the evolution of pixel art as a defining visual style of early arcade games, tracing its roots, exploring its techniques, and discussing the resurgence of pixel art in modern games and digital art.\n\n## Table Summary  \nThe table presents an example of contextual prompting for a language model (gemini-pro) tasked with suggesting blog article topics about retro games. It details the model parameters, the specific prompt used, and the output consisting of three article topic suggestions with descriptions.\n\n## Table Data  \n- **Goal:** Suggest articles for a blog about retro games.\n- **Model:** gemini-pro.\n- **Temperature:** 1.\n- **Top-K:** 40.\n- **Token Limit:** 1024.\n- **Top-P:** 0.8.\n- **Prompt:** Asks the model to suggest 3 topics for a blog about retro 80's arcade video games, each with a short article description.\n- **Output:** Lists three suggested article topics:\n  1. The Evolution of Arcade Cabinet Design (focuses on design changes from the 1970s to the 1980s and beyond).\n  2. Blast From The Past: Iconic Arcade Games of The 80’s (covers history and legacy of influential 1980s arcade games).\n  3. The Rise and Retro Revival of Pixel Art (explores pixel art’s evolution and resurgence in modern digital art).\n\n## Image Description  \nThe image shows a structured table labeled \"Table 7. An example of contextual prompting,\" illustrating the use of a language model to generate creative article suggestions for a blog about retro arcade games. The table includes model parameters, the prompt provided to the model, and detailed output in the form of three suggested article topics with explanations. The layout is clear and easy to read, with sections for goal, model, parameters, prompt, and output.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "24"}, {"data": "## Text Content  \nStep-back prompting\n\nStep-back* prompting is a technique for improving the performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This ‘step back’ allows the LLM to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem.\n\nBy considering the broader and underlying principles, LLMs can generate more accurate and insightful responses. Step-back prompting encourages LLMs to think critically and apply their knowledge in new and creative ways. It changes the final prompt doing the task by utilizing more knowledge in the LLM’s parameters than would otherwise come into play when the LLM is prompted directly.\n\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of specific details, step–back prompting.\n\nLet’s have a look into these examples to understand how step-back prompting can improve the results. Let’s first review a traditional prompt (Table 8) before we compare it to a step back prompt (Table 9):\n\n## Table Summary  \nNo table is present on this page.\n\n## Table Data  \nNo table data is present on this page.\n\n## Image Description  \nThe page contains a clean, text-based layout from a document or book titled \"Prompt Engineering.\" The central content is an explanation of \"Step-back prompting,\" organized in paragraphs for easy reading. There are no visual elements, images, or diagrams aside from the structured text.\n\n## Code  \nNo code is present on this page.\n\n## Formulas  \nNo formulas are present on this page.\n\n## Flowchart Description  \nNo flowchart is present on this page.", "page": "25"}, {"data": "## Text Content  \n\"Write a storyline for a level of a first-person shooter video game.\n\nThe level begins with the player’s squad being ambushed in a dense urban area by a relentless enemy faction. Players must fight their way through a maze of back alleys and dilapidated buildings, utilizing stealth and precision to eliminate enemy forces and avoid detection. The player must use their environment to their advantage, setting traps, utilizing cover, and making calculated moves to survive. Along the way, they uncover intel suggesting the enemy is planning a devastating attack, and the player must race against time to gather evidence and relay it back to their headquarters before the enemy can carry out their plans.\n\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a storyline, but it’s also quite random and generic. So let’s take a step back:\"\n\n## Table Summary  \nThe table presents a traditional prompt engineering setup for generating a first-person shooter game storyline using a large language model, specifying various parameters such as model selection, temperature, token limit, and example prompt/output.\n\n## Table Data  \n- **Goal**: Write a storyline for a level of a first-person shooter video game.\n- **Model**: gemini-pro\n- **Temperature**: 1\n- **Token Limit**: 1024\n- **Top-K**: 40\n- **Top-P**: 0.8\n- **Prompt**: Write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging.\n- **Output**:  \n  The level begins with the player’s squad being ambushed in a dense urban area by a relentless enemy faction. Players must fight their way through a maze of back alleys and dilapidated buildings, utilizing stealth and precision to eliminate enemy forces and avoid detection. The player must use their environment to their advantage, setting traps, utilizing cover, and making calculated moves to survive. Along the way, they uncover intel suggesting the enemy is planning a devastating attack, and the player must race against time to gather evidence and relay it back to their headquarters before the enemy can carry out their plans.\n\n## Image Description  \nThe image consists of a well-structured table illustrating a prompt engineering example for generating a video game storyline, followed by explanatory text. The table includes fields for the goal, model, temperature, token limit, sampling parameters, prompt, and the generated output. Below the table, there is a caption identifying it as a traditional prompt example, and the body text explains the effects of the temperature parameter on output creativity and randomness.\n\n## Code  \n_None present._\n\n## Formulas  \n_None present._\n\n## Flowchart Description  \n_None present._", "page": "26"}, {"data": "## Text Content  \nYeah those topics seem like a good fit for a first-person video game. Let’s go back to the original prompt, but this time we include the answer of the step back as context and see what it will return.\n\n## Table Summary  \nThe table provides an example of prompting for self consistency. It demonstrates how a prompt can be used to generate a list of five fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game.\n\n## Table Data  \n- **Prompt**:  \n  Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game?\n\n- **Output**:  \n  1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.\n  2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.\n  3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.\n  4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.\n  5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.\n\n## Image Description  \nThe image displays a table titled \"Table 9. An example of prompting for self consistency.\" The table contains a prompt about first-person shooter video game settings and provides five detailed fictional environments suitable for engaging level storylines. The visual elements are clean and minimal, focusing on the clear presentation of textual content.\n\n---\n\n*No code, formulas, or flowcharts are present in the image.*", "page": "27"}, {"data": "## Text Content  \nContext: 5 engaging themes for a first person shooter video game:  \n1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.  \n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.  \n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.  \n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.  \n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.  \n\nTake one of the themes and write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging.\n\nIn the heart of a murky abyss, lies a dilapidated underwater research facility, standing as a testament to human ambition and its disastrous consequences. Shrouded in darkness, pulsating with the hum of malfunctioning machinery, and haunted by the echoes of drowned souls, the facility is now a breeding ground for a nightmare borne from the deep sea. The player, an elite marine equipped with advanced diving gear and experimental weaponry, is tasked with venturing into this aquatic realm of terror. Armed with resilience and resourcefulness, they must navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea monstrosities that lurk in the depths. From grappling with the crushing pressure of the deep sea to outsmarting cunning aquatic predators, every moment in this uncharted underworld tests the limits of human endurance and courage.\n\nThat looks like an interesting video game! By using step back prompting techniques you can increase the accuracy of your prompts.\n\n## Table Summary  \nThe table provides an example of prompting for self-consistency in large language models, showcasing how to generate creative and coherent outputs for a video game storyline based on a selected theme.\n\n## Table Data  \n- **Prompt**: Lists five potential first-person shooter settings (Abandoned Military Base, Cyberpunk City, Alien Spaceship, Zombie-Infested Town, Underwater Research Facility) and instructs the user to choose one and create a paragraph-long storyline that is both challenging and engaging.\n- **Output**: Provides a detailed and atmospheric storyline for the \"Underwater Research Facility\" theme, describing the setting, challenges, and emotional tone for the player, emphasizing resilience, resourcefulness, and the dangers of the deep sea.\n\n## Image Description  \nThe image displays a structured table labeled \"Table 10. An example of prompting for self consistency.\" The table is divided into two columns: \"Prompt\" and \"Output.\" The left column presents a context and detailed instructions for a writing task, while the right column contains a rich, atmospheric narrative as the output. The document is visually organized for clarity, with bolded headings and bullet points for easy reading.\n\n## Code  \n_None present in the image._\n\n## Formulas  \n_None present in the image._\n\n## Flowchart Description  \n_None present in the image._", "page": "28"}, {"data": "## Text Content  \nChain of Thought (CoT)\n\nChain of Thought (CoT) prompting is a technique for improving the reasoning capabilities of LLMs by generating **intermediate reasoning steps**. This helps the LLM generate more accurate answers. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding as it’s a challenge with a zero-shot chain of thought.\n\nCoT has a lot of advantages. First of all, it’s low-effort while being very effective and works well with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT prompting, as you can learn from the LLM’s responses and see the reasoning steps that were followed. If there’s a malfunction, you will be able to identify it. Chain of thought appears to improve robustness when moving between different LLM versions. Which means the performance of your prompt should drift less between different LLMs than if your prompt does not use reasoning chains. Of course there are also disadvantages, but they are somewhat intuitive.\n\nThe LLM response includes the chain of thought reasoning, which means more output tokens, which means predictions cost more money and take longer.\n\nTo explain the following example in Table 11, let’s first try to create a prompt that is not using CoT prompting to showcase the flaws of a large language model.\n\n## Table Summary  \nTable 11 provides an example of a straightforward prompt that asks an LLM to solve a mathematical problem without using chain of thought reasoning. It demonstrates the kind of answer produced by a model when intermediate reasoning steps are not provided.\n\n## Table Data  \n- **Prompt**:  \n  \"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\"\n- **Output**:  \n  \"63 years old\"\n\n## Image Description  \nThe page contains a styled section titled \"Chain of Thought (CoT)\", with an explanation of the CoT prompting technique, its advantages, disadvantages, and an example table (Table 11) illustrating a direct prompt and the corresponding output from a language model. There are no additional graphics or illustrations on the page.\n\n## Formulas  \nNo explicit mathematical formulas are present in the image.\n\n## Flowchart Description  \nThere is no flowchart present in the image.", "page": "29"}, {"data": "## Text Content  \nYikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with mathematical tasks and can provide incorrect answers – even for a task as simple as multiplying two numbers. This is because they are trained on large volumes of text and math may require a different approach. So let’s see if intermediate reasoning steps will improve the output.\n\nTable 12. An example of Chain of Thought prompting\n\nAlright, now the final answer is correct. That’s because we clearly instructed the LLM to explain each step instead of just returning an answer. It’s interesting to see that the model adds up the 17 increased years. In my head I would take the difference of years between my partner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more like me.\n\n## Table Summary  \nThe table demonstrates how using a step-by-step, \"Chain of Thought\" prompting method enables a language model (LLM) to solve a mathematical word problem correctly by breaking down the reasoning process into logical steps.\n\n## Table Data  \n- **Prompt:**  \n  \"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\"\n\n- **Output:**  \n  1. Let’s assume my current age is “x” years.\n  2. When I was 3 years old, my age was 3 years.\n  3. My partner’s age at that time was 3 times my age, which means my partner’s age was 3 * 3 = 9 years.\n  4. Now, I am 20 years old, which means my age has increased by 20 – 3 = 17 years since I was 3 years old.\n  5. Since my partner’s age was 9 years when I was 3 years old, and my age has increased by 17 years, my partner’s age must have also increased by 17 years.\n  6. Therefore, my partner’s current age is 9 + 17 = 26 years.\n  - So, my partner is 26 years old.\n\n## Image Description  \nThe image displays a page from a document on prompt engineering, specifically discussing how language models handle mathematical reasoning. It features a table titled \"An example of Chain of Thought prompting,\" showing how detailed step-by-step instructions enable LLMs to arrive at the correct answer for a word problem involving ages. Additional explanatory text below the table clarifies the reasoning behind the correct answer and reflects on the difference between the model's and a human's approach to the problem.\n\n## Formulas  \n\\[\n\\text{Partner's age when I was 3} = 3 \\times 3 = 9\n\\]\n\\[\n\\text{Years passed} = 20 - 3 = 17\n\\]\n\\[\n\\text{Partner's current age} = 9 + 17 = 26\n\\]\n\\[\n\\text{Alternate human calculation: } 20 + (9 - 3) = 26\n\\]\n\n## Flowchart Description  \nThere is no explicit flowchart depicted in the image. However, the step-by-step reasoning shown in the table is analogous to a linear flow of logical steps, each building on the previous, to reach the final answer.", "page": "30"}, {"data": "## Text Content  \nTable 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be very powerful when combined with a single-shot or few-shot, as what you can see in Table 13:\n\nChain of thought can be useful for various use-cases. Think of code generation, for breaking down the request into a few steps, and mapping those to specific lines of code. Or for creating synthetic data when you have some kind of seed like “The product is called XYZ, write a description guiding the model through the assumptions you would make based on the product given title.” Generally, any task that can be solved by ‘talking through’ is a good candidate for a chain of thought. If you can explain the steps to solve the problem, try chain of thought.\n\nPlease refer to the notebook hosted in the GoogleCloudPlatform Github repository which will go into further detail on CoT prompting:\n\n## Table Summary  \nTable 13 presents an example of a chain of thought (CoT) prompting using a single-shot approach. The table demonstrates how step-by-step reasoning can be employed to solve age-related logic problems, providing both the prompt and the resulting output.\n\n## Table Data  \n- **Prompt:**  \n  - Q: When my brother was 2 years old, I was double his age. Now I am 40 years old. How old is my brother? Let's think step by step.  \n  - A: When my brother was 2 years, I was 2 × 2 = 4 years old. That's an age difference of 2 years and I am older. Now I am 40 years old, so my brother is 40 - 2 = 38 years old. The answer is 38.  \n  - Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.  \n  - A:\n- **Output:**  \n  - When I was 3 years old, my partner was 3 × 3 = 9 years old. That’s an age difference of 6 years and my partner is older. Now I am 20 years old, so my partner is 20 + 6 = 26 years old. The answer is 26.\n\n## Image Description  \nThe image shows a section from a document about prompt engineering, specifically focusing on chain of thought (CoT) prompting. It contains a table illustrating how to use CoT with a single-shot example to solve age-difference problems step by step. The layout is clean and text-based, emphasizing the logical progression in problem-solving.\n\n## Code  \n*(No code is present in the image.)*\n\n## Formulas  \n- Age difference calculation:  \n  \\( \\text{Older person's age} - \\text{Younger person's age} = \\text{Age difference} \\)\n- Partner's age calculation:  \n  \\( 3 \\times 3 = 9 \\)  \n  \\( 20 + 6 = 26 \\)\n\n## Flowchart Description  \n*(No flowchart is present in the image.)*", "page": "31"}, {"data": "## Text Content  \nIn the best practices section of this chapter, we will learn some best practices specific to Chain of thought prompting.\n\n### Self-consistency\n\nWhile large language models have shown impressive success in various NLP tasks, their ability to reason is often seen as a limitation that cannot be overcome solely by increasing model size. As we learned in the previous Chain of Thought prompting section, the model can be prompted to generate reasoning steps like a human solving a problem. However CoT uses a simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer. It improves the accuracy and coherence of responses generated by LLMs.\n\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but obviously has high costs.\n\nIt follows the following steps:\n\n1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple times. A high temperature setting encourages the model to generate different reasoning paths and perspectives on the problem.\n2. Extract the answer from each generated response.\n3. Choose the most common answer.\n\nLet’s look into an example of an email classification system, which classifies an email as IMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM multiple times, to see if the responses differ after each submit. Notice the friendly tone, the word choice and the sarcasm that’s been used in the email. All this could trick the LLM.", "page": "32"}, {"data": "## Text Content  \nEMAIL:  \n```\nHi,\nI have seen you use Wordpress for your website. A great open source content management system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy to set up.\n\nI did notice a bug in the contact form, which happens when you select the name field. See the attached screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\n\nBut for the rest it's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more interesting things to read.\n\nCheers,\nHarry the Hacker.\n```\nClassify the above email as IMPORTANT or NOT IMPORTANT. Let's think step by step and explain why.\n\n## Table Summary  \nThis table presents a prompt for an email classification task. The email describes the discovery of a bug in a Wordpress contact form, provides specific details about the bug, and asks whether the email should be classified as IMPORTANT or NOT IMPORTANT, with an explanation required.\n\n## Table Data  \n- The table contains a single prompt for an email classification task.\n- The prompt includes the full text of an email from \"Harry the Hacker\" who claims to have found a JavaScript bug in the website's contact form.\n- The sender describes how the bug manifests (a JavaScript alert box appears upon entering text in the name field) and mentions an attached screenshot.\n- The sender concludes with a lighthearted remark, suggesting to leave the bug in place for their amusement.\n- The prompt instructs the reader to classify the email as IMPORTANT or NOT IMPORTANT, and to think step by step and explain the reasoning.\n\n## Image Description  \nThe image shows a page from a document titled \"Prompt Engineering.\" The main content is a table containing a prompt. The prompt presents an email, formatted as a block of text, and a question asking the reader to classify the importance of the email and explain their reasoning. The page footer indicates it is from February 2025, page 33.\n\n## Code  \n```\nEMAIL:\nHi,\nI have seen you use Wordpress for your website. A great open source content management system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy to set up.\n\nI did notice a bug in the contact form, which happens when you select the name field. See the attached screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\n\nBut for the rest it's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more interesting things to read.\n\nCheers,\nHarry the Hacker.\n```\n\n## Formulas  \nNo mathematical formulas are present in this image.\n\n## Flowchart Description  \nNo flowcharts are present in this image.", "page": "33"}, {"data": "## Text Content  \n**Output Attempt 1**\n\n**Step 1: Identify the purpose of the email.**  \nThe purpose of the email is to inform the recipient of a bug in the contact form on their website. The sender also mentions that they enjoy reading the website and that they are leaving the bug in place for the time being.\n\n**Step 2: Assess the potential impact of the bug.**  \nThe bug in the contact form could allow an attacker to execute arbitrary JavaScript code on the website. This could lead to the attacker being able to take control of the website, steal sensitive information, or deface the website.\n\n**Step 3: Consider the credibility of the sender.**  \nThe sender of the email claims to be a hacker named Harry. However, there is no way to verify the sender’s identity. It is possible that the sender is a legitimate hacker who is trying to help the recipient, but it is also possible that the sender is a malicious actor who is trying to trick the recipient into clicking on a malicious link or downloading a malicious file.\n\n**Conclusion: IMPORTANT**  \nBased on the potential impact of the bug and the credibility of the sender, the email should be classified as \"**IMPORTANT**\". The recipient should take immediate action to fix the bug and protect their website from attack.\n\n**Output Attempt 2**\n\n**Explanation:**\n1. **Lack of urgency:** The email does not convey any sense of urgency or time-sensitivity. It starts with a casual greeting and presents the information in a matter-of-fact tone.\n2. **Non-critical bug report:** The bug reported in the contact form is described as interesting but not critical. The email sender even suggests that the website owner can leave the bug in place.\n3. **Lack of personal impact:** The email does not indicate any direct or personal impact on the website owner. It simply mentions that the sender enjoys reading the website.\n\n*Continues next page...*\n\n## Table Summary  \nThe table compares two different output attempts that analyze the same email. Output Attempt 1 follows a structured risk assessment process and emphasizes the importance of the report. Output Attempt 2 focuses on the tone and content of the email, suggesting it is not urgent or critical.\n\n## Table Data  \n- **Output Attempt 1:**  \n  - Analyzes the email in three steps: identifying the purpose, assessing impact, and considering sender credibility.\n  - Concludes that the email is \"IMPORTANT\" and recommends immediate action due to the potential security risk.\n\n- **Output Attempt 2:**  \n  - Lists three points: lack of urgency, non-critical nature of the bug, and no personal impact.\n  - Suggests the email does not require urgent action and is not critical, emphasizing the sender's casual tone.\n\n## Image Description  \nThe image displays a two-column table. The left column lists \"Output Attempt 1\" and \"Output Attempt 2.\" The right column provides detailed written evaluations for each attempt. The content discusses how to classify an email reporting a bug on a website, considering urgency, severity, and sender credibility. The bottom of the image includes a footer with the date \"February 2025\" and the page number \"34.\" The top of the page is titled \"Prompt Engineering.\"\n\n## Code  \nN/A\n\n## Formulas  \nN/A\n\n## Flowchart Description  \nN/A", "page": "34"}, {"data": "## Text Content  \n\"4. **Absence of action request:** The email does not explicitly request any action from the website owner. It does not ask for the bug to be fixed or provide any specific suggestions for improvement.\n\n5. **Sender’s intent:** The sender’s intention seems to be more focused on sharing an observation and expressing interest in the website rather than raising a serious concern or requesting assistance.\n\n**Conclusion: NOT IMPORTANT**\n\n**Step 1: Identify the purpose of the email.**  \nThe purpose of the email is to inform the recipient of a bug in the contact form on their website. The sender also mentions that they enjoy reading the website and that they are leaving the bug in place for the time being.\n\n**Step 2: Assess the potential impact of the bug.**  \nThe bug in the contact form could allow an attacker to execute arbitrary JavaScript code on the website. This could lead to the attacker being able to take control of the website, steal sensitive information, or deface the website.\n\n**Step 3: Consider the credibility of the sender.**  \nThe sender of the email claims to be a hacker named Harry. However, there is no way to verify the sender’s identity. It is possible that the sender is a legitimate hacker who is trying to help the recipient, but it is also possible that the sender is a malicious actor who is trying to exploit the bug for their own gain.\n\n**Conclusion:**  \nBased on the above factors, the email should be classified as **IMPORTANT**.  \nThe bug in the contact form is a serious security risk, and the sender’s credibility is unknown. The recipient should take immediate action to fix the bug and\"\n\n\"You can use the above prompt and try to see if it returns a consistent classification.  \nDepending on the model that you are using and the temperature configuration, it might return “IMPORTANT” or “NOT IMPORTANT”.\"\n\n## Table Summary  \nThe table presents two different output attempts for classifying the importance of an email reporting a website bug. Each attempt uses a distinct reasoning process to arrive at either \"IMPORTANT\" or \"NOT IMPORTANT\" as the conclusion.\n\n## Table Data  \n- **Output Attempt 2:**  \n  - Notes the absence of an explicit action request in the email, and interprets the sender's intent as more observational and interested rather than urgent or assistance-seeking.\n  - **Conclusion:** NOT IMPORTANT\n\n- **Output Attempt 3:**  \n  - Breaks down the reasoning into three steps:\n    1. Identifies the purpose of the email—to inform about a bug in the contact form.\n    2. Assesses the potential impact—the bug allows for possible execution of arbitrary JavaScript code, which could threaten website security.\n    3. Considers the credibility of the sender—acknowledges the sender may be a hacker whose intent is unclear.\n  - **Conclusion:** IMPORTANT, due to the seriousness of the security risk and uncertainty about the sender's motives.\n\n## Image Description  \nThe image is a screenshot from a document discussing prompt engineering in AI, specifically showing a table (Table 14) that compares two different attempts at classifying the importance of an email about a website bug. The table illustrates how different reasoning steps can lead to different conclusions (“IMPORTANT” or “NOT IMPORTANT”) for the same scenario. Below the table, there is explanatory text suggesting that the prompt can be used with AI models to test for consistency in classification outcomes.\n\n## Code  \n_None present in the image._\n\n## Formulas  \n_None present in the image._\n\n## Flowchart Description  \n_None present in the image._", "page": "35"}, {"data": "## Text Content  \nBy generating many Chains of Thoughts, and taking the most commonly occurring answer (“IMPORTANT”), we can get a more consistently correct answer from the LLM.\n\nThis example shows how self-consistency prompting can be used to improve the accuracy of an LLM’s response by considering multiple perspectives and selecting the most consistent answer.\n\n### Tree of Thoughts (ToT)\n\nNow that we are familiar with chain of thought and self-consistency prompting, let’s review Tree of Thoughts (ToT). It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. This is depicted in Figure 1.\n\n## Image Description  \nThe image displays two diagrams side by side:\n\n- **Left Diagram**: Illustrates the traditional \"chain of thought\" prompting, where a single linear pathway connects an \"Input\" box at the top through a series of sequential steps (represented by gray boxes) to an \"Output\" box at the bottom.\n- **Right Diagram**: Depicts the \"Tree of Thoughts\" (ToT) method. An \"Input\" box at the top branches out into multiple pathways, forming a hierarchical tree structure. Each branch splits into several nodes (green and red boxes) representing different reasoning paths. Ultimately, several of these branches converge towards the \"Output\" box at the bottom, showing how different reasoning paths can be explored and considered in parallel.\n\nFigure caption: \"Figure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on the right\"\n\n## Flowchart Description  \n- **Chain of Thought Prompting (Left)**:  \n  - Starts with an \"Input\".\n  - Proceeds through a single, linear sequence of reasoning steps.\n  - Ends at an \"Output\".\n\n- **Tree of Thoughts Prompting (Right)**:  \n  - Begins with an \"Input\".\n  - Branches into multiple reasoning paths (nodes).\n  - Each node may further branch, forming a tree-like structure.\n  - Multiple reasoning paths are explored simultaneously.\n  - Paths converge towards the \"Output\", allowing for consideration of various perspectives before arriving at a final answer.", "page": "36"}, {"data": "## Text Content  \nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It works by maintaining a tree of thoughts, where each thought represents a coherent language sequence that serves as an intermediate step toward solving a problem. The model can then explore different reasoning paths by branching out from different nodes in the tree.\n\nThere’s a great notebook, which goes into a bit more detail showing The Tree of Thought (ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’.\n\n### ReAct (reason & act)\n\nReason and act (ReAct) prompting is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling.\n\nReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct performs well against other prompt engineering approaches in a variety of domains.\n\nReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem.\n\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain framework for Python, together with VertexAI (google-cloud-aiplatform) and the google-search-results pip packages.\n\n## Table Summary  \n*There are no tables present in the image.*\n\n## Table Data  \n*No table data to display.*\n\n## Image Description  \n*The image consists solely of text content, section headers, and some inline code references. There are no illustrations or visual elements beyond the textual layout.*\n\n## Code  \n```\ngoogle-cloud-aiplatform\ngoogle-search-results\n```\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are depicted or referenced in the image.*", "page": "37"}, {"data": "## Text Content  \nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-api-key and set an environment variable SERPAPI_API_KEY.\n\nNext let’s write some Python code, with the task for the LLM to figure out: *How many children have a famous dad that performs in the band Metallica?*\n\nSnippet 1. Creating a ReAct Agent with LangChain and VertexAI\n\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact, the LLM is scraping Google search results to figure out the band names. Then, it lists the results as observations and chains the thought for the next search.\n\n## Code  \n```python\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import VertexAI\n\nprompt = \"How many kids do the band members of Metallica have?\"\n\nllm = VertexAI(temperature=0.1)\ntools = load_tools([\"serpapi\"], llm=llm)\n\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(prompt)\n```\n\n## Image Description  \nThe image contains a screenshot of a Python code block, demonstrating how to create a ReAct agent using LangChain and VertexAI. The code is focused on answering the question about how many kids the members of the band Metallica have, by leveraging an LLM and the SerpAPI tool for web searches.\n\n## Table Summary  \n*No table content appears in this image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Formulas  \n*No mathematical formulas are present in this image.*\n\n## Flowchart Description  \n*No flowchart is present in this image.*", "page": "38"}, {"data": "## Text Content  \nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches each band member to request the total of children and add up the total. Finally, it returns the total number of children as the final answer.\n\nReAct prompting in practice requires understanding that you continually have to resend the previous prompts/responses (and do trimming of the extra generated content) as well as set up the model with appropriate examples/instructions. Please refer to the notebook^4 hosted in the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the actual LLM inputs and outputs with a more elaborate example.\n\n## Code  \n```\n> Entering new AgentExecutor chain...\nMetallica has 4 members.\nAction: Search\nAction Input: How many kids does James Hetfield have?\nObservation: three children\nThought: 1/4 Metallica band members have 3 children\nAction: Search\nAction Input: How many kids does Lars Ulrich have?\nObservation: 3\nThought: 2/4 Metallica band members have 6 children\nAction: Search\nAction Input: How many kids does Kirk Hammett have?\nObservation: Hammett has been married to his second wife Lani since 1998. They have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June 28, 2008).\nThought: 3/4 Metallica band members have 8 children\nAction: Search\nAction Input: How many kids does Robert Trujillo have?\nObservation: 2\nThought: 4/4 Metallica band members have 10 children\nFinal Answer: 10\n```\n\n## Image Description  \nThe image shows a code output in a terminal where an agent executes a series of search actions to determine how many children each of the four Metallica band members have. The process is stepwise:  \n1. The agent first identifies there are four band members.  \n2. For each member—James Hetfield, Lars Ulrich, Kirk Hammett, and Robert Trujillo—it searches for and records the number of children.  \n3. After each search, it updates the cumulative total of children.  \n4. The agent's final answer, after summing the children for all four members, is 10.\n\n## Table Summary  \n*No formal table is present in the image, but the code block effectively acts as a stepwise log or pseudo-table, detailing the process and tally.*\n\n## Table Data  \n- **James Hetfield**: 3 children  \n- **Lars Ulrich**: 3 children  \n- **Kirk Hammett**: 2 children  \n- **Robert Trujillo**: 2 children  \n- **Total**: 10 children\n\n## Flowchart Description  \nThe agent's process can be described as a sequential flow:  \n1. Identify the total number of band members.  \n2. For each band member, perform a search query to find out the number of children.  \n3. Add each result to a cumulative total.  \n4. After all members have been processed, return the final total.\n\n## Formulas  \nNo explicit mathematical formulas are present in the image. However, the calculation can be expressed as:  \n\\[\n\\text{Total children} = 3 + 3 + 2 + 2 = 10\n\\]", "page": "39"}, {"data": "## Text Content  \nAutomatic Prompt Engineering\n\nAt this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to automate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt Engineering (APE). This method not only alleviates the need for human input but also enhances the model’s performance in various tasks.\n\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good ones. And repeat.\n\nFor example, you can use automatic prompt engineering that will help with training a chatbot for a merchandise t-shirt webshop. We want to figure out all the various ways customers could phrase their order for buying a band merchandise t-shirt.\n\n1. Write the prompt which will generate the output variants. In this example, I am using gemini-pro to generate 10 instructions. See Table 15:\n\n## Table Summary  \n*No table data is present on this page, but the text references \"Table 15\" for a future example of prompt outputs.*\n\n## Table Data  \n*No table is shown on this page.*\n\n## Image Description  \n*This page is a clean, text-based document with the header \"Prompt Engineering\" and a section titled \"Automatic Prompt Engineering.\" The layout is minimal with centered headings and structured paragraphs. There is no illustrative or graphical content.*\n\n## Code  \n*No code is present on this page.*\n\n## Formulas  \n*No formulas are present on this page.*\n\n## Flowchart Description  \n*No flowchart is present on this page.*", "page": "40"}, {"data": "## Text Content  \nWe have a band merchandise t-shirt webshop, and to train a chatbot we need various ways to order: \"One Metallica t-shirt size S\". Generate 10 variants, with the same semantics but keep the same meaning.\n\n1. I'd like to purchase a Metallica t-shirt in size small.\n2. Can I order a small-sized Metallica t-shirt?\n3. I'd like to buy one Metallica t-shirt in small.\n4. One Metallica shirt, size small, please.\n5. I want one Metallica t-shirt in size small.\n6. I'd like to place an order for one Metallica t-shirt in small.\n7. Small Metallica t-shirt, one please.\n8. I'm looking for one Metallica t-shirt in small.\n9. Can you get me one Metallica t-shirt in small?\n10. One Metallica small t-shirt, please.\n\n2. Evaluate all of the instruction candidates by scoring the candidates based on a chosen metric. For example, you can use BLEU (Bilingual Evaluation Understudy) or ROUGE (Recall-Oriented Understudy for Gisting Evaluation).\n\n3. Select the instruction candidate with the highest evaluation score. This candidate will be the final prompt you can use in your software application or chatbot. You can also tweak the selected prompt and evaluate again.\n\n## Table Summary  \nThe table demonstrates how a large language model (LLM) can be used to generate multiple prompt variants for ordering a specific item from a webshop. The example focuses on rephrasing the request for a \"Metallica t-shirt size S\" in 10 different ways, all with the same meaning.\n\n## Table Data  \n- **Prompt**:  \n  \"We have a band merchandise t-shirt webshop, and to train a chatbot we need various ways to order: 'One Metallica t-shirt size S'. Generate 10 variants, with the same semantics but keep the same meaning.\"\n- **Output (10 Variants)**:  \n  1. I'd like to purchase a Metallica t-shirt in size small.\n  2. Can I order a small-sized Metallica t-shirt?\n  3. I'd like to buy one Metallica t-shirt in small.\n  4. One Metallica shirt, size small, please.\n  5. I want one Metallica t-shirt in size small.\n  6. I'd like to place an order for one Metallica t-shirt in small.\n  7. Small Metallica t-shirt, one please.\n  8. I'm looking for one Metallica t-shirt in small.\n  9. Can you get me one Metallica t-shirt in small?\n  10. One Metallica small t-shirt, please.\n\n## Image Description  \nThe image displays a section from a document on prompt engineering, including a table that illustrates how to use an LLM to generate multiple prompt variants for ordering a specific product online. The table lists an initial prompt and then provides 10 alternative phrasings for the same order intent. Below the table, the document explains steps for evaluating and selecting the best instruction candidate for chatbot training.\n\n## Code  \n_None present in the image._\n\n## Formulas  \n_None present in the image._\n\n## Flowchart Description  \n_None present in the image._", "page": "41"}, {"data": "## Text Content  \nCode prompting\n\nGemini focuses primarily on text-based prompts, which also includes writing prompts for returning code. Let’s go to the Vertex AI Studio and test these prompts to look at some coding examples.\n\nPrompts for writing code\n\nGemini can also be a developer and help you with writing code in any programming language of choice. As a developer this can help you speed up the process of writing code.\n\nImagine a folder on your machine with hundreds of files that needs to be renamed. Renaming each file would take you a lot of time. You know a little Bash, and could write a script to automate this, but that may also take awhile. So let’s write a prompt. You can write the prompt in the public consumer chatbot Gemini, or if you are more concerned about confidentiality, you can write these prompts within your Google Cloud account and open Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the temperature etc.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No table data to display.*\n\n## Image Description  \nThe image is a page from a document titled \"Prompt Engineering.\" The content focuses on using Gemini for code prompting, specifically how Gemini can assist in generating code through prompts. It describes scenarios where code automation is helpful, such as renaming multiple files, and explains how Gemini or Vertex AI Studio can be used to generate such code. The page is cleanly formatted with section headers for \"Code prompting\" and \"Prompts for writing code,\" followed by explanatory paragraphs.\n\n## Code  \n*No code blocks are present in the image.*\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "42"}, {"data": "## Text Content  \nWrite a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name draft to the file name.\n\nThat seems like good code to me – it’s even documented! However, since LLMs can’t reason, and repeat training data, it’s essential to read and test your code first.\n\nThe moment we are all waiting for, does it really work?\n\n## Table Summary  \nThe table describes a prompt engineering example for using Gemini to program code. It specifies the goal, model, parameters, and provides both a prompt and the resulting Bash script output for renaming files in a folder.\n\n## Table Data  \n- **Name**: 1_bash_rename_files_in_folder  \n- **Goal**: Write a prompt to write code in Bash to rename files in a folder.  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 1024  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt**:  \n  - Write a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name draft to the file name.  \n- **Output**:  \n  - Bash script that prompts the user for a folder name, checks if the folder exists, lists the files, and renames each one by prepending \"draft_\" to the filename.\n\n## Code  \n```bash\n#!/bin/bash\n# Ask for the folder name\necho \"Enter the folder name: \"\nread folder_name\n\n# Check if the folder exists\nif [ ! -d \"$folder_name\" ]; then\n  echo \"Folder does not exist.\"\n  exit 1\nfi\n# Get the list of files in the folder\nfiles=( \"$folder_name\"/* )\n\n# Rename each file by prepending \"draft\" to the file name\nfor file in \"${files[@]}\"; do\n  new_file_name=\"draft_$(basename \"$file\")\"\n  mv \"$file\" \"$new_file_name\"\ndone\n\necho \"Files renamed successfully.\"\n```\n\n## Image Description  \nThe image is a screenshot of a prompt engineering example. It displays a table (Table 16: Using Gemini to program code) showing the process of generating a Bash script via a prompt to an LLM. Below the table, there is explanatory text discussing the reliability of LLM-generated code and emphasizing the importance of testing the output.\n\n## Formulas  \n*No mathematical formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "43"}, {"data": "## Text Content  \nLet’s try it first with a test folder with only a few files in it, which will need to be renamed from filename.txt to draft_filename.txt.\n\n1. Copy the output from Table 16 (without the ```bash ``` text wrapper), and paste it in a new file called: “rename_files.sh”.\n2. Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g. test. and hit enter.\n3. The script seems to run fine. You will see the message: **Files renamed successfully.** When you look into the test folder, notice that all the files were perfectly renamed to **draft_filename.txt**.\n\nIt worked!\n\n---\n\n### Prompts for explaining code\n\nAs a developer when you work in teams you have to read someone else’s code. Gemini can help you with this as well. Let’s take the code output from Table 16, remove the comments and ask the large language model to explain what’s happening. See Table 17:\n\n\n## Table Summary  \n*No table is present in this image. Table 16 and Table 17 are referenced but not shown.*\n\n## Table Data  \n*No table data is present in this image.*\n\n## Image Description  \nThis page is a text-based instructional page from a guide on prompt engineering. It provides step-by-step instructions for testing a file renaming script and introduces the concept of using AI (Gemini) to explain code in a collaborative development environment. There are no illustrations, diagrams, or other visual elements apart from text.\n\n## Code  \n*No code is shown in this image. The instructions reference code from Table 16, but do not display the code itself.*\n\n## Formulas  \n*No formulas are present in this image.*\n\n## Flowchart Description  \n*No flowcharts are present in this image.*", "page": "44"}, {"data": "## Text Content  \nPrompt Engineering\n\nName: 1_bash_explain_the_code  \nGoal: Write a prompt to explain Bash code  \nModel: gemini-pro  \nTemperature: 0.1  \nToken Limit: 1024  \nTop-K: N/A  \nTop-P: 1  \n\nPrompt:  \nExplain to me the below Bash code:  \n...\n\nOutput:  \nThis Bash script performs the following tasks:\n\n1. **User Input**:  \n   - It prompts the user to enter a folder name using the `echo` and `read` commands. The entered folder name is stored in the `folder_name` variable.\n\n2. **Folder Existence Check**:  \n   - It checks if the folder with the specified name exists using the `[ ! -d \"$folder_name\" ]` condition. If the folder does not exist, it displays an error message (“Folder does not exist.”) and exits the script with an exit code of 1.\n\nContinues next page...\n\n## Table Summary  \nThe table provides metadata and detailed information about a prompt designed to explain Bash code, including the prompt’s name, goal, model, and parameters.\n\n## Table Data  \n- **Prompt Name**: 1_bash_explain_the_code  \n- **Goal**: To write a prompt that explains Bash code.  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 1024  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt Content**: Requests an explanation for a specific Bash script.  \n- **Output Content**: Provides a stepwise breakdown of the Bash script’s functionality, emphasizing user input handling and folder existence checking.\n\n## Image Description  \nThe image displays a structured table summarizing the parameters and prompt for explaining Bash code. Below the table, it shows a Bash script and the corresponding explanation of its tasks, specifically focusing on user input and folder existence checking. The explanation is organized in a bulleted and numbered format for clarity. The page is labeled \"Prompt Engineering\" and is dated February 2025, page 45.\n\n## Code  \n```bash\n#!/bin/bash\necho \"Enter the folder name: \"\nread folder_name\n\nif [ ! -d \"$folder_name\" ]; then\n  echo \"Folder does not exist.\"\n  exit 1\nfi\n\nfiles=( \"$folder_name/\"* )\n\nfor file in \"${files[@]}\"; do\n  new_file_name=draft_$(basename \"$file\")\n  mv \"$file\" \"$new_file_name\"\ndone\n\necho \"Files renamed successfully.\"\n```\n", "page": "45"}, {"data": "## Text Content  \nPrompts for translating code\n\nThe bash code from Table 16 seems to work fine. However this script could really be reusable when it would prompt me for file names, ideally it should work as a separate application with an UI. As a starting point, Python would be a better language for a (web) application than Bash. LLMs can help with translating code from one language to another. See the example in Table 18.\n\n## Table Summary  \nTable 17 provides a detailed prompt for explaining code, specifically focused on describing the steps involved in a script that lists files in a folder, renames them, and displays a success message.\n\n## Table Data  \n- **File Listing**:  \n  The script lists all the files inside a specified folder using the command `files=(\"${folder_name}/\"*)`. This command creates an array named `files` that contains the paths to all files in the folder.\n\n- **File Renaming**:  \n  The script iterates through each file in the `files` array using a `for` loop. For each file, it generates a new file name with the command `new_file_name=\"draft_$(basename \"$file\")\"`, which prefixes the original file name with \"draft_\". Then, it renames the file using the command `mv \"$file\" \"$new_file_name\"`, effectively moving the file to its new name.\n\n- **Success Message**:  \n  After all files have been renamed, the script displays the message \"Files renamed successfully.\" to indicate the operation was completed without errors.\n\n## Image Description  \nThe image shows a section of a document on prompt engineering. It includes a boxed table (Table 17) that details the steps for explaining a file-renaming script, followed by a section header and paragraph discussing the value of translating Bash scripts into more user-friendly formats, such as Python, with the help of large language models (LLMs).\n\n## Code  \n_No code is directly shown in the image. Descriptions of code commands and logic are provided in the table._\n\n## Formulas  \n_No mathematical formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "46"}, {"data": "## Text Content  \nTranslate the below Bash code to a Python snippet.\n\n## Table Summary  \nThe table provides configuration details for a prompt engineering task that translates Bash code to Python using the gemini-pro model, including prompt parameters and an example input-output.\n\n## Table Data  \n- **Name**: 1_python_rename_files  \n- **Goal**: Write a prompt to translate Bash code to Python  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 1024  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt**:  \n  - Instructs to translate a Bash script that renames files in a folder to a Python snippet. The Bash script:\n    - Prompts user for a folder name.\n    - Checks if the folder exists; exits if not.\n    - Iterates over files in the folder, prefixes each filename with \"draft_\", and renames them.\n    - Prints a success message.\n- **Output**:  \n  - Python code that replicates the Bash script's functionality:\n    - Imports `os` and `shutil`.\n    - Prompts the user for a folder name.\n    - Checks if the folder exists; prints a message and exits if not.\n\n## Image Description  \nThe image displays a table with configuration and prompt details for a prompt engineering task. It includes a Bash script for renaming files and a corresponding Python script as output. The table also specifies model parameters such as temperature, token limit, and model name (gemini-pro). The lower right corner notes the page number and date.\n\n## Code  \n**Bash Code in Prompt:**  \n```bash\n#!/bin/bash\n\necho \"Enter the folder name: \"\nread folder_name\n\nif [ ! -d \"$folder_name\" ]; then\n  echo \"Folder does not exist.\"\n  exit 1\nfi\n\nfiles=( \"$folder_name\"/* )\nfor file in \"${files[@]}\"; do\n  new_file_name=\"draft_$(basename \"$file\")\"\n  mv \"$file\" \"$new_file_name\"\ndone\necho \"Files renamed successfully.\"\n```\n\n**Python Code in Output:**  \n```python\nimport os\nimport shutil\n# Get the folder name from the user\nfolder_name = input(\"Enter the folder name: \")\n\n# Check if the folder exists\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n```\n(Note: The Python code is truncated with \"Continues next page...\")", "page": "47"}, {"data": "## Text Content  \nRead and review the code. Copy the output from the prompt and paste it into a new file: **file_renamer.py**. Test the code by opening a Terminal window, and execute the following command `python file_renamer.py`.\n\n**NOTE:** When prompting for (Python) code in the Language Studio in Vertex AI, you will have to click on the 'Markdown' button. Otherwise you will receive plain text which is missing the proper indenting of lines, which is important for running Python code.\n\n### Prompts for debugging and reviewing code\n\nLet’s manually write some edits to the code of Table 18. It should prompt the user for the filename prefix, and write this prefix in upper case characters. See the example code in Snippet *3, but what a bummer. It now returns Python errors!\n\n## Table Summary  \nThe table provides a prompt for translating code from Bash to Python, specifically for renaming files in a folder with a given prefix.\n\n## Table Data  \n- The output code demonstrates how to:\n  - List all files in a specified folder.\n  - Iterate over each file.\n  - Create a new file name with the prefix \"draft_\".\n  - Move (rename) the file to the new name.\n  - Print a success message after completion.\n\n## Code  \n```\n# Get a list of all files in the folder\nfiles = os.listdir(folder_name)\n\n# Iterate over the files\nfor file in files:\n    # Create a new file name with the prefix \"draft_\"\n    new_file_name = f\"draft_{file}\"\n\n    # Move the file to the new name\n    shutil.move(os.path.join(folder_name, file),\n                os.path.join(folder_name, new_file_name))\n\n# Print a success message\nprint(\"Files renamed successfully.\")\n...\n```\n\n## Image Description  \nThe image shows a page from a guide on prompt engineering. It features a boxed section containing Python code for renaming files, a table caption (\"A prompt for translating code from Bash to Python\"), and instructional text below. The text instructs the reader to copy and run the code, points out the importance of code formatting in Vertex AI, and introduces a new section about debugging and editing Python code to prompt for user input and handle filename prefixes.\n\n## Formulas  \nNo mathematical formulas are present in the image.\n\n## Flowchart Description  \nNo flowcharts are present in the image.", "page": "48"}, {"data": "## Text Content  \nPython\n\nfolder_name = input(\"Enter the folder name: \")\nprefix = input(\"Enter the string to prepend to the filename: \")\ntext = toUpperCase(prefix)\n\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n\nfiles = os.listdir(folder_name)\n\nfor file in files:\n    new_filename = f\"{text}_{file}\"\n\nshutil.move(os.path.join(folder_name, file),\n            os.path.join(folder_name, new_filename))\n\nprint(\"Files renamed successfully.\")\n\nSnippet 3. A broken Python script\n\nDoh! That looks like a bug:\n\nThe below Python code gives an error:\n\nTraceback (most recent call last):\n  File \"/Users/leeboonstra/Documents/test_folder/rename_files.py\", line 7, in <module>\n    text = toUpperCase(prefix)\nNameError: name ‘toUpperCase’ is not defined\n\nSnippet 4. I broke the Python code\n\nLet’s see if we can ask the large language model to debug and review code. Have a look into Table 19:\n\n## Table Summary  \n*There is no table present in this image.*\n\n## Table Data  \n*No table data is present.*\n\n## Image Description  \nThe image is a screenshot from a document discussing prompt engineering, showing a code snippet inside a rounded gray box labeled \"Python.\" Below, there is a highlighted error traceback box, followed by explanatory text. The document footer includes the date \"February 2025\" and a page number.\n\n## Code  \n```python\nimport os\nimport shutil\n\nfolder_name = input(\"Enter the folder name: \")\nprefix = input(\"Enter the string to prepend to the filename: \")\ntext = toUpperCase(prefix)\n\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n\nfiles = os.listdir(folder_name)\n\nfor file in files:\n    new_filename = f\"{text}_{file}\"\n\nshutil.move(os.path.join(folder_name, file),\n            os.path.join(folder_name, new_filename))\n\nprint(\"Files renamed successfully.\")\n```\n\n```plaintext\nTraceback (most recent call last):\n  File \"/Users/leeboonstra/Documents/test_folder/rename_files.py\", line 7, in <module>\n    text = toUpperCase(prefix)\nNameError: name ‘toUpperCase’ is not defined\n```\n\n## Formulas  \n*No mathematical formulas are present.*\n\n## Flowchart Description  \n*No flowchart is present in this image.*", "page": "49"}, {"data": "## Text Content  \nThe below Python code gives an error:\n\nTraceback (most recent call last):\n  File \"/Users/leeboonstra/Documents/test_folder/rename_files.py\", line 7, in <module>\n    text = toUpperCase(prefix)\nNameError: name 'toUpperCase' is not defined\n\nDebug what's wrong and explain how I can improve the code.\n\n## Table Summary  \nThis table presents prompt engineering parameters for a task involving debugging and reviewing Python code, including meta-information such as the prompt name, goal, model, and tuning parameters.\n\n## Table Data  \n- **Name**: 1_python_debug_code  \n- **Goal**: Write a prompt to debug and review Python code.  \n- **Model**: gemini-pro  \n- **Temperature**: 0.1  \n- **Token Limit**: 1024  \n- **Top-K**: N/A  \n- **Top-P**: 1  \n- **Prompt**: Includes a Python code snippet that raises a NameError and asks for debugging and code improvement advice.\n\n## Image Description  \nThe image is a page from a document about prompt engineering. It contains a table summarizing a prompt for debugging Python code, followed by the prompt text and a code snippet. The code generates an error due to the use of an undefined function, `toUpperCase`. The page footer notes \"February 2025\" and the page number \"50.\"\n\n## Code  \n```python\nimport os\nimport shutil\n\n# Get the folder name from the user\nfolder_name = input(\"Enter the folder name: \")\nprefix = input(\"Enter the string to prepend to the filename: \")\ntext = toUpperCase(prefix)\n\n# Check if the folder exists\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n\n# Get a list of all files in the folder\nfiles = os.listdir(folder_name)\n\n# Iterate over the files\nfor file in files:\n```\n\n## Formulas  \nNo mathematical formulas are present in the image.\n\n## Flowchart Description  \nNo flowcharts are present in the image.", "page": "50"}, {"data": "## Text Content  \nThe code calls the 'toUpperCase' function to convert 'prefix' to uppercase, but that function is not defined. To fix this issue, you can use the 'upper()' method of the string class which converts a given string into uppercase. The modified code is shown below:\n\n\"Folder does not exist.\"\n\n\"Files renamed successfully.\"\n\nContinues next page...\n\n## Table Summary  \nThis table illustrates a code prompt asking for a script to rename files with a prefix and provides a corrected code implementation in the output, addressing an error regarding string case conversion.\n\n## Table Data  \n- **Prompt**:  \n  - Create a new file name with the prefix \"draft_\":  \n    `new_filename = f\"{text}_{file}\"`\n  - Move the file to the new name:  \n    `shutil.move(os.path.join(folder_name, file), os.path.join(folder_name, new_file_name))`\n  - Print a success message:  \n    `print(\"Files renamed successfully.\")`\n\n- **Output**:  \n  - The original code incorrectly calls 'toUpperCase' to convert 'prefix' to uppercase; this function is not defined in Python.\n  - Correction: Use the string method '.upper()' to convert the prefix to uppercase.\n  - The provided corrected Python code:\n    - Imports necessary modules (`os`, `shutil`).\n    - Prompts the user for the folder name and prefix.\n    - Converts the prefix to uppercase using '.upper()'.\n    - Checks if the folder exists and lists files.\n    - Iterates over files, prepends the uppercase prefix, and renames the files.\n    - Prints appropriate messages for folder existence and successful renaming.\n\n## Image Description  \nThe image displays a two-column table labeled \"Prompt\" and \"Output.\" The \"Prompt\" section contains a code snippet for renaming files by adding a prefix. The \"Output\" section provides a detailed explanation, identifies a bug in the original code, and presents a corrected Python code block with comments and logical structure. The page footer includes \"February 2025\" and the page number \"51\".\n\n## Code  \n```python\nimport os\nimport shutil\n\n# Get the folder name from the user\nfolder_name = input(\"Enter the folder name: \")\nprefix = input(\"Enter the string to prepend to the filename: \")\ntext = prefix.upper()\n\n# Check if the folder exists\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n\n# Get a list of all files in the folder\nfiles = os.listdir(folder_name)\n\n# Iterate over the files\nfor file in files:\n    # Create a new file name with the prefix \"draft_\"\n    new_filename = f\"{text}_{file}\"\n\n    # Move the file to the new name\n    shutil.move(os.path.join(folder_name, file),\n```\n\n## Formulas  \nNone.\n\n## Flowchart Description  \nNone.", "page": "51"}, {"data": "## Text Content  \nAdditionally, there are a few other improvements that can be made to the code:  \n1. The file extension of the new filenames is not kept. It’s better to extract the extension from the original filename and append it to the new filename.  \n2. The code assumes that there are no spaces in the folder name. However, it is a good practice to handle spaces gracefully by replacing them with underscores or hyphens.  \n3. Using the `f` string syntax for string interpolation is more readable and concise than the traditional `+` operator.  \n4. The code doesn’t handle errors that might occur during the renaming process. It would be better to wrap the `shutil.move` call in a `try...except` block to catch any potential errors.\n\nHere is the improved code with these suggestions:\n\n## Code\n```python\nimport os\nimport shutil\n# Get the folder name from the user\nfolder_name = input('Enter the folder name: ')\nprefix = input('Enter the string to prepend to the filename: ')\n# Check if the folder exists\nif not os.path.isdir(folder_name):\n    print('Folder does not exist.')\n    exit(1)\n# Get a list of all files in the folder\nfiles = os.listdir(folder_name)\n\n# Iterate over the files\nfor file in files:\n```\n*(The code continues on the next page)*\n\n## Table Summary  \nNo formal table is present; the content is structured as explanatory text with a numbered list and code.\n\n## Table Data  \nNo conventional table data is present.\n\n## Image Description  \nThe image is a screenshot or digital page from a document about \"Prompt Engineering.\" It displays a section labeled \"Output\" with Python code and suggestions for improving code that renames files in a folder. The page includes a numbered list of best practices for file handling in Python and an improved code snippet that incorporates these suggestions. There's also a \"Continues next page...\" note indicating that the code example continues on the following page.\n\n## Formulas  \nNo mathematical formulas are present.\n\n## Flowchart Description  \nNo flowcharts are present.", "page": "52"}, {"data": "## Text Content  \nThese improvements make the code more robust and flexible while also making it easier to read and understand... (The response was truncated because it has reached the token limit. Try to increase the token limit if you need a longer response.)\n\nThat’s great. Not only did it tell me how to solve the problem, it also figured out that my code had more bugs and how to solve them, too. The last part of the prompt gave suggestions to improve the code in general.\n\n## Table Summary  \nTable 19 presents a prompt for debugging and reviewing Python code, showing an example of output generated by an AI assistant that analyzes and rewrites a Python script for renaming files more robustly.\n\n## Table Data  \n- The table provides Python code for renaming files:\n  - It extracts the file extension.\n  - Creates a new filename with a prefix and the original extension.\n  - Constructs full paths for both the old and new filenames.\n  - Attempts to move (rename) the file, handling errors if they occur.\n  - Prints a success message if files are renamed successfully.\n- The table notes that these improvements make the code more robust, flexible, and easier to read.\n- The response in the table was truncated due to reaching a token limit, with a suggestion to increase the limit for longer responses.\n\n## Image Description  \nThe image shows a boxed output section containing a Python code snippet for renaming files, followed by an explanation of the improvements. Below the box, there's a caption for Table 19, which states it demonstrates a prompt for debugging and reviewing Python code. The surrounding text assesses the AI's ability to analyze, solve, and improve code, noting that it found additional bugs and provided general suggestions for improvement.\n\n## Code  \n```python\n# Get the file extension\next = os.path.splitext(file)[1]\n\n# Create a new file name with the prefix and the file extension\nnew_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"\n\n# Build the full paths for the original and new files\nold_path = os.path.join(folder_name, file)\nnew_path = os.path.join(folder_name, new_filename)\n\n# Try to move the file\ntry:\n    shutil.move(old_path, new_path)\nexcept Exception as e:\n    print(f\"An error occurred while moving the file {file}: {e}\")\n\n# Print a success message\nprint(\"Files renamed successfully.\")\n```\n(Note: The code was truncated in the image, indicated by the \"...\")\n\n## Flowchart Description  \nNo flowchart is present in the image.\n\n## Formulas  \nNo mathematical formulas are present in the image.", "page": "53"}, {"data": "## Text Content  \nWhat about multimodal prompting?\n\nPrompting for code still uses the same regular large language model. Multimodal prompting is a separate concern, it refers to a technique where you use multiple input formats to guide a large language model, instead of just relying on text. This can include combinations of text, images, audio, code, or even other formats, depending on the model’s capabilities and the task at hand.\n\nBest Practices\n\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to play around with your prompts, with the ability to test against the various models.\n\nUse the following best practices to become a pro in prompt engineering.\n\nProvide examples\n\nThe most important best practice is to provide (one shot / few shot) examples within a prompt. This is highly effective because it acts as a powerful teaching tool. These examples showcase desired outputs or similar responses, allowing the model to learn from them and tailor its own generation accordingly. It’s like giving the model a reference point or target to aim for, improving the accuracy, style, and tone of its response to better match your expectations.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe image is a clean, white page from a document or presentation about \"Prompt Engineering.\" It contains structured headings and paragraphs discussing multimodal prompting and best practices for prompt engineering. The style uses bold section headers and regular body text, with no diagrams, tables, or illustrations present.\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "54"}, {"data": "## Text Content  \nPrompt Engineering\n\nDesign with simplicity\n\nPrompts should be concise, clear, and easy to understand for both you and the model. As a rule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try not to use complex language and don’t provide unnecessary information.\n\nExamples:\n\nBEFORE:  \nI am visiting New York right now, and I’d like to hear more about great locations. I am with two 3 year old kids. Where should we go during our vacation?\n\nAFTER REWRITE:  \nAct as a travel guide for tourists. Describe great places to visit in New York Manhattan with a 3 year old.\n\nTry using verbs that describe the action. Here’s a set of examples:\n\n**Act, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, Summarize, Translate, Write.**\n\nFebruary 2025\n\n## Table Summary  \nNo tables are present in the image.\n\n## Table Data  \nNo table data to report.\n\n## Image Description  \nThe image is a clean, single-page instructional document on prompt engineering. It explains the importance of designing prompts with simplicity, using clear and concise language. The page presents an example of a verbose prompt and its rewritten, simplified version. It also provides a list of recommended verbs to use when crafting prompts for language models. The layout is minimal, with bold headers and select bolded words for emphasis.\n\n## Code  \nNo code is present in the image.\n\n## Formulas  \nNo mathematical formulas are present in the image.\n\n## Flowchart Description  \nNo flowcharts are present in the image.", "page": "55"}, {"data": "## Text Content  \nBe specific about the output\n\nBe specific about the desired output. A concise instruction might not guide the LLM enough or could be too generic. Providing specific details in the prompt (through system or context prompting) can help the model to focus on what’s relevant, improving the overall accuracy.\n\nExamples:\n\nDO:\n\nGenerate a 3 paragraph blog post about the top 5 video game consoles.  \nThe blog post should be informative and engaging, and it should be written in a conversational style.\n\nDO NOT:\n\nGenerate a blog post about video game consoles.\n\nUse Instructions over Constraints\n\nInstructions and constraints are used in prompting to guide the output of a LLM.\n\n- An **instruction** provides explicit instructions on the desired format, style, or content of the response. It guides the model on what the model should do or produce.\n\n- A **constraint** is a set of limitations or boundaries on the response. It limits what the model should not do or avoid.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No table data to extract.*\n\n## Image Description  \nThe page is a clean instructional document on prompt engineering. It uses bold section headings and bullet points to distinguish between definitions and examples. The layout emphasizes clarity, with clear separation between sample instructions and explanations about their use. The lower part of the page uses bullet points to explain the difference between \"instructions\" and \"constraints.\"\n\n## Code  \n*No code is present in the image.*\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "56"}, {"data": "## Text Content  \nGrowing research suggests that focusing on positive instructions in prompting can be more effective than relying heavily on constraints. This approach aligns with how humans prefer positive instructions over lists of what not to do.\n\nInstructions directly communicate the desired outcome, whereas constraints might leave the model guessing about what is allowed. It gives flexibility and encourages creativity within the defined boundaries, while constraints can limit the model's potential. Also a list of constraints can clash with each other.\n\nConstraints are still valuable but in certain situations. To prevent the model from generating harmful or biased content or when a strict output format or style is needed.\n\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to do instead. This can avoid confusion and improve the accuracy of the output.\n\nDO:\n\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Only discuss the console, the company who made it, the year, and total sales.\n\nDO NOT:\n\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Do not list video game names.\n\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model to do and only use constraints when necessary for safety, clarity or specific requirements. Experiment and iterate to test different combinations of instructions and constraints to find what works best for your specific tasks, and document these.\n\n## Table Summary  \n*No tables are present in the image.*\n\n## Table Data  \n*No tables are present in the image.*\n\n## Image Description  \nThe image is a page from a book or document titled \"Prompt Engineering.\" It contains a section of text discussing the effectiveness of positive instructions versus constraints in prompt design for AI models. The page includes a clear example of \"DO\" and \"DO NOT\" instructions and concludes with best practices for balancing instructions and constraints.\n\n## Code  \n```\nGenerate a 1 paragraph blog post about the top 5 video game consoles.\nOnly discuss the console, the company who made it, the year, and total sales.\n```\n```\nGenerate a 1 paragraph blog post about the top 5 video game consoles.\nDo not list video game names.\n```\n\n## Formulas  \n*No formulas are present in the image.*\n\n## Flowchart Description  \n*No flowcharts are present in the image.*", "page": "57"}, {"data": "## Text Content  \nControl the max token length\n\nTo control the length of a generated LLM response, you can either set a max token limit in the configuration or explicitly request a specific length in your prompt. For example:\n\n*\"Explain quantum physics in a tweet length message.\"*\n\nUse variables in prompts\n\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be changed for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a city. Instead of hardcoding the city name in the prompt, use a variable. Variables can save you time and effort by allowing you to avoid repeating yourself. If you need to use the same piece of information in multiple prompts, you can store it in a variable and then reference that variable in each prompt. This makes a lot of sense when integrating prompts into your own applications.\n\n## Table Summary  \nThe table demonstrates how to use variables in prompts to create dynamic and reusable instructions for language models, specifically by setting a variable for a city name and generating a fact about that city.\n\n## Table Data  \n- **Prompt Section:**\n  - VARIABLES  \n    {city} = \"Amsterdam\"\n  - PROMPT  \n    You are a travel guide. Tell me a fact about the city: {city}\n\n- **Output Section:**  \n  Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a great place to visit for its rich history, culture, and nightlife.\n\n## Image Description  \nThe image features a page from a document discussing prompt engineering techniques for large language models. It focuses on methods to control the length of model outputs and describes the use of variables in prompts for flexibility and reusability. A sample table illustrates how a variable (city name) can be used in a prompt to generate tailored output, with \"Amsterdam\" as the example.\n\n## Code  \n```\nVARIABLES\n{city} = \"Amsterdam\"\nPROMPT\nYou are a travel guide. Tell me a fact about the city: {city}\n```\n", "page": "58"}, {"data": "## Text Content  \nExperiment with input formats and writing styles\n\nDifferent models, model configurations, prompt formats, word choices, and submits can yield different results. Therefore, it’s important to experiment with prompt attributes like the style, the word choice, and the type prompt (zero shot, few shot, system prompt).\n\nFor example a prompt with the goal to generate text about the revolutionary video game console Sega Dreamcast, can be formulated as a question, a statement or an instruction, resulting in different outputs:\n\n- **Question:** What was the Sega Dreamcast and why was it such a revolutionary console?\n- **Statement:** The Sega Dreamcast was a sixth-generation video game console released by Sega in 1999. It...\n- **Instruction:** Write a single paragraph that describes the Sega Dreamcast console and explains why it was so revolutionary.\n\nFor few-shot prompting with classification tasks, mix up the classes\n\nGenerally speaking, the order of your few-shots examples should not matter much. However, when doing classification tasks, make sure you mix up the possible response classes in the few shot examples. This is because you might otherwise be overfitting to the specific order of the examples. By mixing up the possible response classes, you can ensure that the model is learning to identify the key features of each class, rather than simply memorizing the order of the examples. This will lead to more robust and generalizable performance on unseen data.\n\n## Table Summary  \n_No tables are present in the image._\n\n## Table Data  \n_No table data to extract._\n\n## Image Description  \nThe image is a page from a document or book on \"Prompt Engineering.\" It consists of section headers, explanatory paragraphs, and a bulleted list. The page discusses strategies for experimenting with prompt formats and styles, especially in the context of language models. It also provides guidance for few-shot prompting with classification tasks, emphasizing the importance of mixing up class order in examples to avoid overfitting.\n\n## Code  \n_No code blocks present in the image._\n\n## Formulas  \n_No mathematical formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "59"}, {"data": "## Text Content  \nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy from there.\n\n### Adapt to model updates\n\nIt’s important for you to stay on top of model architecture changes, added data, and capabilities. Try out newer model versions and adjust your prompts to better leverage new model features. Tools like Vertex AI Studio are great to store, test, and document the various versions of your prompt.\n\n### Experiment with output formats\n\nBesides the prompt input format, consider experimenting with the output format. For non-creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try having your output returned in a structured format like JSON or XML.\n\nThere are some benefits in returning JSON objects from a prompt that extracts data. In a real-world application I don’t need to manually create this JSON format, I can already return the data in a sorted order (very handy when working with datetime objects), but most importantly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations.\n\nIn summary, benefits of using JSON for your output:\n- Returns always in the same style\n- Focus on the data you want to receive\n\n## Table Summary  \n_No tables are present in the image._\n\n## Table Data  \n_No table data is available in the image._\n\n## Image Description  \nThe page appears to be from a guide or manual on \"Prompt Engineering,\" specifically focusing on adapting to model updates and experimenting with output formats. The layout is clean and textual, with section headers in bold. There are no illustrations, diagrams, or visual elements besides text.\n\n## Code  \n_No code is present in the image._\n\n## Formulas  \n_No formulas are present in the image._\n\n## Flowchart Description  \n_No flowcharts are present in the image._", "page": "60"}, {"data": "## Text Content  \n- Less chance for hallucinations\n- Make it relationship aware\n- You get data types\n- You can sort it\n\nTable 4 in the few-shot prompting section shows an example on how to return structured output.\n\n### JSON Repair\n\nWhile returning data in JSON format offers numerous advantages, it's not without its drawbacks. The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs. Furthermore, JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits. This truncation often results in invalid JSON, missing crucial closing braces or brackets, rendering the output unusable. Fortunately, tools like the `json-repair` library (available on PyPI) can be invaluable in these situations. This library intelligently attempts to automatically fix incomplete or malformed JSON objects, making it a crucial ally when working with LLM-generated JSON, especially when dealing with potential truncation issues.\n\n## Table Summary  \nNo tables are present in the image.\n\n## Table Data  \nNo table data is present in the image.\n\n## Image Description  \nThe page is a text-heavy document discussing prompt engineering, specifically the advantages and challenges of using JSON for structured data output. There are no illustrations or visual diagrams present.\n\n## Code  \nNo executable code blocks are present, but there is a reference to the `json-repair` library.\n\n## Formulas  \nNo mathematical formulas are present in the image.\n\n## Flowchart Description  \nNo flowcharts are present in the image.", "page": "61"}, {"data": "## Text Content  \nWorking with Schemas\n\nUsing structured JSON as an output is a great solution, as we've seen multiple times in this paper. But what about input? While JSON is excellent for structuring the output the LLM generates, it can also be incredibly useful for structuring the input you provide. This is where JSON Schemas come into play. A JSON Schema defines the expected structure and data types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input. Furthermore, schemas can help establish relationships between different pieces of data and even make the LLM \"time-aware\" by including date or timestamp fields with specific formats.\n\nHere's a simple example:\n\nLet's say you want to use an LLM to generate descriptions for products in an e-commerce catalog. Instead of just providing a free-form text description of the product, you can use a JSON schema to define the product's attributes:\n\nSnippet 5. Definition of the structured output schema\n\nFebruary 2025\n\n## Code  \n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": { \"type\": \"string\", \"description\": \"Product name\" },\n    \"category\": { \"type\": \"string\", \"description\": \"Product category\" },\n    \"price\": { \"type\": \"number\", \"format\": \"float\", \"description\": \"Product price\" },\n    \"features\": {\n      \"type\": \"array\",\n      \"items\": { \"type\": \"string\" },\n      \"description\": \"Key features of the product\"\n    },\n    \"release_date\": { \"type\": \"string\", \"format\": \"date\", \"description\": \"Date the product was released\"}\n  }\n}\n```\n\n## Image Description  \nThe image consists of a clean, text-based instructional page from a technical document or book. It is titled \"Working with Schemas\" and explains the value of using JSON schemas to structure both input and output for large language models (LLMs). The page provides a practical example of a JSON schema for defining product attributes in an e-commerce catalog, with a code block illustrating the schema. The layout is visually organized, with bolded section headers, explanatory text, and a highlighted code snippet.\n\n## Table Summary  \n_No tables are present in this image._\n\n## Table Data  \n_No tables are present in this image._\n\n## Formulas  \n_No formulas are present in this image._\n\n## Flowchart Description  \n_No flowcharts are present in this image._", "page": "62"}, {"data": "## Text Content  \nThen, you can provide the actual product data as a JSON object that conforms to this schema:\n\nSnippet 6. Structured output from the LLM\n\nBy preprocessing your data and instead of providing full documents only providing both the schema and the data, you give the LLM a clear understanding of the product’s attributes, including its release date, making it much more likely to generate an accurate and relevant description. This structured input approach, guiding the LLM’s attention to the relevant fields, is especially valuable when working with large volumes of data or when integrating LLMs into complex applications.\n\n**Experiment together with other prompt engineers**\n\nIf you are in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt. When everyone follows the best practices (as listed in this chapter) you are going to see a variance in performance between all the different prompt attempts.\n\n## Code  \n```json\n{\n  \"name\": \"Wireless Headphones\",\n  \"category\": \"Electronics\",\n  \"price\": 99.99,\n  \"features\": [\"Noise cancellation\", \"Bluetooth 5.0\", \"20-hour battery life\"],\n  \"release_date\": \"2023-10-27\"\n}\n```", "page": "63"}, {"data": "## Text Content\n\n**CoT Best practices**\n\nFor CoT prompting, putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer.\n\nWith CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning.\n\nFor CoT prompting, set the temperature to 0.\n\nChain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the language model. Generally speaking, when using reasoning, to come up with the final answer, there’s likely one single correct answer. Therefore the temperature should always set to 0.\n\n**Document the various prompt attempts**\n\nThe last tip was mentioned before in this chapter, but we can’t stress enough how important it is: document your prompt attempts in full detail so you can learn over time what went well and what did not.\n\nPrompt outputs can differ across models, across sampling settings, and even across different versions of the same model. Moreover, even across identical prompts to the same model, small differences in output sentence formatting and word choice can occur. (For example, as mentioned previously, if two tokens have the same predicted probability, ties may be broken randomly. This can then impact subsequent predicted tokens.).\n\n## Table Summary\n\nNo tables are present in the image.\n\n## Table Data\n\nNo tables are present in the image.\n\n## Image Description\n\nThe image is a page from a document or book on prompt engineering. The page is text-based and includes two main sections: \"CoT Best practices\" and \"Document the various prompt attempts.\" There are no diagrams, illustrations, or visual elements other than the text.\n\n## Code\n\nNo code content is present in the image.\n\n## Formulas\n\nNo mathematical formulas are present in the image.\n\n## Flowchart Description\n\nNo flowcharts are present in the image.", "page": "64"}, {"data": "## Text Content  \nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of this approach are that you have a complete record when you inevitably have to revisit your prompting work—either to pick it up in the future (you’d be surprised how much you can forget after just a short break), to test prompt performance on different versions of a model, and to help debug future errors.\n\nBeyond the fields in this table, it’s also helpful to track the version of the prompt (iteration), a field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture feedback. If you’re lucky enough to be using Vertex AI Studio, save your prompts (using the same name and version as listed in your documentation) and track the hyperlink to the saved prompt in the table. This way, you’re always one click away from re-running your prompts.\n\nWhen working on a retrieval augmented generation system, you should also capture the specific aspects of the RAG system that impact what content was inserted into the prompt, including the query, chunk settings, chunk output, and other information.\n\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the codebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally your prompts are part of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation procedures to understand how well your prompt generalizes to a task.\n\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s performance. Keep experimenting until you achieve the desired output. When you change a model or model configuration, go back and keep experimenting with the previously used prompts.\n\n## Table Summary  \nNo table is present in this image. The text refers to \"Table 21\" as a template for organizing prompt engineering experiments, but the actual table is not included.\n\n## Table Data  \nNo direct table data is present in the image. The text suggests tracking prompt versions, results (OK/NOT OK/SOMETIMES OK), feedback, and links to saved prompts in a table format.\n\n## Image Description  \nThere are no illustrations or diagrams in this image. The content is a page of text describing best practices for prompt engineering workflow documentation and iteration.\n\n## Code  \nNo code is present in this image.\n\n## Formulas  \nNo formulas are present in this image.\n\n## Flowchart Description  \nNo flowcharts are present in this image.", "page": "65"}, {"data": "## Text Content  \nThis whitepaper discusses prompt engineering. We learned various prompting techniques, such as:\n\n- Zero prompting\n- Few shot prompting\n- System prompting\n- Role prompting\n- Contextual prompting\n- Step-back prompting\n- Chain of thought\n- Self consistency\n- Tree of thoughts\n\n## Table Summary  \nThe table provides a structured template for documenting prompts used in prompt engineering, capturing key parameters and specifications for each prompt attempt.\n\n## Table Data  \n- **Name**: [name and version of your prompt]  \n- **Goal**: [One sentence explanation of the goal of this attempt]  \n- **Model**: [name and version of the used model]  \n- **Temperature**: [value between 0 - 1]  \n- **Token Limit**: [number]  \n- **Top-K**: [number]  \n- **Top-P**: [number]  \n- **Prompt**: [Write all the full prompt]  \n- **Output**: [Write out the output or multiple outputs]  \n\nThe table is titled \"A template for documenting prompts\" and is intended to help users clearly record and organize the details and results of their prompt engineering experiments.\n\n## Image Description  \nThe page contains a clean layout with a table at the top, titled as a template for documenting prompts. Below the table, a \"Summary\" section appears, listing various prompting techniques with bullet points. The overall design is minimal and organized for easy reference in a technical or academic document. There are no additional images or illustrations besides the structured text and table.", "page": "66"}, {"data": "## Text Content  \nPrompt Engineering\n\n- ReAct\n\nWe even looked into ways how you can automate your prompts.\n\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen when your prompts are insufficient. We closed with best practices on how to become a better prompt engineer.\n\nFebruary 2025\n\n## Table Summary  \n_No tables present in the image._\n\n## Table Data  \n_No table data available._\n\n## Image Description  \nThe image is a plain document page with the section title \"Prompt Engineering\" at the top left. The content consists of a bullet point about \"ReAct,\" followed by a brief discussion on automating prompts and addressing challenges in generative AI. The footer contains the date \"February 2025\" and the page number \"67.\"\n\n## Code  \n_No code present in the image._\n\n## Formulas  \n_No formulas present in the image._\n\n## Flowchart Description  \n_No flowcharts present in the image._", "page": "67"}, {"data": "## Text Content  \nEndnotes\n\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com.\n\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at: https://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html.\n\n3. Google Cloud, 2023, Introduction to Prompting. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design.\n\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body.\n\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at: https://arxiv.org/pdf/2109.01652.pdf.\n\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden.\n\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at: https://arxiv.org/pdf/2005.14165.pdf.\n\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. Available at: https://openreview.net/pdf?id=3bq3sjvcQ1.\n\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf.\n\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb.\n\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models. Available at: https://arxiv.org/pdf/2203.11171.pdf.\n\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Available at: https://arxiv.org/pdf/2305.10601.pdf.\n\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at: https://arxiv.org/pdf/2210.03629.pdf.\n\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/advanced_prompting_training/cot_react.ipynb.\n\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt Engineers. Available at: https://arxiv.org/pdf/2211.01910.pdf.\n\nFebruary 2025\n\n## Table Summary  \n_No table content is present in the image._\n\n## Table Data  \n_None._\n\n## Image Description  \nThe image displays the \"Endnotes\" section from a document titled \"Prompt Engineering.\" It lists 15 numbered references, each citing a publication, web page, or research paper relevant to prompt engineering and large language models, with URLs provided for further reading. The page footer notes \"February 2025\" and the page number \"68.\"\n\n## Code  \n_None._\n\n## Formulas  \n_None._\n\n## Flowchart Description  \n_None._", "page": "68"}]